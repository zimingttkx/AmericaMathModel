{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 问题1: 三种方案综合对比分析\n",
    "\n",
    "本notebook对比分析三种粉丝投票估算方案的性能：\n",
    "- **方案A**: 线性规划/约束满足模型\n",
    "- **方案B**: 贝叶斯MCMC模型\n",
    "- **方案C**: 遗传算法模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 环境配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from scipy.optimize import minimize\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "matplotlib.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "SCIENTIFIC_COLORS = ['#E64B35', '#4DBBD5', '#00A087', '#3C5488', '#F39B7F', '#8491B4']\n",
    "MODEL_COLORS = {'A': '#E64B35', 'B': '#4DBBD5', 'C': '#00A087'}\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "np.random.seed(42)\n",
    "print('环境配置完成')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('../../data/processed/粉丝投票分析.xlsx')\n",
    "print(f'数据维度: {df.shape}')\n",
    "\n",
    "def prepare_week_data(df, season, week):\n",
    "    week_df = df[(df['赛季'] == season) & (df['第几周'] == week)].copy()\n",
    "    if len(week_df) == 0:\n",
    "        return None\n",
    "    return {\n",
    "        'contestants': week_df['选手姓名'].tolist(),\n",
    "        'judge_scores': week_df['本周评委总分'].values,\n",
    "        'judge_pct': week_df['评委百分比'].values,\n",
    "        'eliminated': week_df[week_df['是否被淘汰'] == 1]['选手姓名'].tolist(),\n",
    "        'method': week_df['排名方法'].iloc[0]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 定义三种模型（简化版）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方案A: 线性规划\n",
    "def model_a_solve(week_data):\n",
    "    contestants = week_data['contestants']\n",
    "    judge_pct = np.array(week_data['judge_pct'])\n",
    "    eliminated = week_data['eliminated']\n",
    "    n = len(contestants)\n",
    "    \n",
    "    if n == 0 or len(eliminated) == 0:\n",
    "        return None, 0\n",
    "    \n",
    "    elim_idx = [contestants.index(e) for e in eliminated if e in contestants]\n",
    "    surv_idx = [i for i in range(n) if i not in elim_idx]\n",
    "    \n",
    "    if len(elim_idx) == 0 or len(surv_idx) == 0:\n",
    "        return None, 0\n",
    "    \n",
    "    def objective(f):\n",
    "        return np.sum((f - 100/n) ** 2)\n",
    "    \n",
    "    constraints = [{'type': 'eq', 'fun': lambda f: np.sum(f) - 100}]\n",
    "    for e_idx in elim_idx:\n",
    "        for s_idx in surv_idx:\n",
    "            def constraint(f, e=e_idx, s=s_idx):\n",
    "                return (judge_pct[s] + f[s]) - (judge_pct[e] + f[e]) - 0.1\n",
    "            constraints.append({'type': 'ineq', 'fun': constraint})\n",
    "    \n",
    "    bounds = [(0, 100) for _ in range(n)]\n",
    "    f0 = np.ones(n) * (100 / n)\n",
    "    result = minimize(objective, f0, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "    \n",
    "    if result.success:\n",
    "        return dict(zip(contestants, result.x)), 1.0\n",
    "    return None, 0\n",
    "\n",
    "# 方案B: MCMC\n",
    "def model_b_solve(week_data, n_samples=300):\n",
    "    n = len(week_data['contestants'])\n",
    "    if n == 0 or len(week_data['eliminated']) == 0:\n",
    "        return None, 0\n",
    "    \n",
    "    samples = [np.random.dirichlet(np.ones(n)) * 100 for _ in range(n_samples)]\n",
    "    samples = np.array(samples)\n",
    "    mean_votes = np.mean(samples, axis=0)\n",
    "    std_votes = np.std(samples, axis=0)\n",
    "    certainty = 1 - np.mean(std_votes / (mean_votes + 1e-6))\n",
    "    \n",
    "    return dict(zip(week_data['contestants'], mean_votes)), max(0, certainty)\n",
    "\n",
    "# 方案C: 遗传算法\n",
    "def model_c_solve(week_data, pop_size=30, n_gen=50):\n",
    "    contestants = week_data['contestants']\n",
    "    n = len(contestants)\n",
    "    if n == 0 or len(week_data['eliminated']) == 0:\n",
    "        return None, 0\n",
    "    \n",
    "    def fitness(ind):\n",
    "        fan_pct = ind / np.sum(ind) * 100\n",
    "        total = week_data['judge_pct'] + fan_pct\n",
    "        elim_idx = [contestants.index(e) for e in week_data['eliminated']]\n",
    "        sorted_idx = np.argsort(total)\n",
    "        return sum(1 for e in elim_idx if e in sorted_idx[:len(elim_idx)]) / len(elim_idx)\n",
    "    \n",
    "    population = [np.random.dirichlet(np.ones(n)) * 100 for _ in range(pop_size)]\n",
    "    best = None\n",
    "    best_fit = -1\n",
    "    \n",
    "    for gen in range(n_gen):\n",
    "        fits = [fitness(ind) for ind in population]\n",
    "        max_idx = np.argmax(fits)\n",
    "        if fits[max_idx] > best_fit:\n",
    "            best_fit = fits[max_idx]\n",
    "            best = population[max_idx].copy()\n",
    "        \n",
    "        new_pop = [best]\n",
    "        for _ in range(pop_size - 1):\n",
    "            p1, p2 = population[np.random.randint(pop_size)], population[np.random.randint(pop_size)]\n",
    "            child = 0.5 * p1 + 0.5 * p2 + np.random.normal(0, 2, n)\n",
    "            child = np.maximum(child, 0)\n",
    "            child = child / np.sum(child) * 100\n",
    "            new_pop.append(child)\n",
    "        population = new_pop\n",
    "    \n",
    "    return dict(zip(contestants, best)), best_fit\n",
    "\n",
    "print('三种模型定义完成')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 运行对比测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seasons = [1, 2, 5, 10, 27, 28, 34]\n",
    "results = {'A': [], 'B': [], 'C': []}\n",
    "times = {'A': [], 'B': [], 'C': []}\n",
    "\n",
    "import time\n",
    "\n",
    "print('开始对比测试...')\n",
    "for season in test_seasons:\n",
    "    season_df = df[df['赛季'] == season]\n",
    "    weeks = sorted(season_df['第几周'].unique())\n",
    "    \n",
    "    for week in weeks[:5]:  # 每季测试前5周\n",
    "        week_data = prepare_week_data(df, season, week)\n",
    "        if week_data is None or len(week_data['eliminated']) == 0:\n",
    "            continue\n",
    "        \n",
    "        # 方案A\n",
    "        t0 = time.time()\n",
    "        res_a, score_a = model_a_solve(week_data)\n",
    "        times['A'].append(time.time() - t0)\n",
    "        results['A'].append({'season': season, 'week': week, 'score': score_a if res_a else 0})\n",
    "        \n",
    "        # 方案B\n",
    "        t0 = time.time()\n",
    "        res_b, score_b = model_b_solve(week_data)\n",
    "        times['B'].append(time.time() - t0)\n",
    "        results['B'].append({'season': season, 'week': week, 'score': score_b})\n",
    "        \n",
    "        # 方案C\n",
    "        t0 = time.time()\n",
    "        res_c, score_c = model_c_solve(week_data)\n",
    "        times['C'].append(time.time() - t0)\n",
    "        results['C'].append({'season': season, 'week': week, 'score': score_c})\n",
    "\n",
    "print(f'测试完成! 共测试 {len(results[\"A\"])} 个周次')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 可视化: 三种方案性能对比（分组柱状图）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# 按赛季汇总\n",
    "season_scores = {s: {'A': [], 'B': [], 'C': []} for s in test_seasons}\n",
    "for model in ['A', 'B', 'C']:\n",
    "    for r in results[model]:\n",
    "        season_scores[r['season']][model].append(r['score'])\n",
    "\n",
    "x = np.arange(len(test_seasons))\n",
    "width = 0.25\n",
    "\n",
    "for i, model in enumerate(['A', 'B', 'C']):\n",
    "    means = [np.mean(season_scores[s][model]) if season_scores[s][model] else 0 for s in test_seasons]\n",
    "    stds = [np.std(season_scores[s][model]) if len(season_scores[s][model]) > 1 else 0 for s in test_seasons]\n",
    "    bars = ax.bar(x + i*width, means, width, yerr=stds, label=f'Model {model}', \n",
    "                  color=MODEL_COLORS[model], capsize=3, alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Season', fontsize=12)\n",
    "ax.set_ylabel('Performance Score', fontsize=12)\n",
    "ax.set_title('Three Models Performance Comparison by Season', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels([f'S{s}' for s in test_seasons])\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 可视化: 运行时间对比（箱线图）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 左图: 运行时间箱线图\n",
    "ax1 = axes[0]\n",
    "bp = ax1.boxplot([times['A'], times['B'], times['C']], labels=['Model A\\n(LP)', 'Model B\\n(MCMC)', 'Model C\\n(GA)'],\n",
    "                 patch_artist=True, showmeans=True)\n",
    "for i, (box, model) in enumerate(zip(bp['boxes'], ['A', 'B', 'C'])):\n",
    "    box.set_facecolor(MODEL_COLORS[model])\n",
    "ax1.set_ylabel('Time (seconds)', fontsize=11)\n",
    "ax1.set_title('(A) Computation Time Comparison', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 右图: 平均时间柱状图\n",
    "ax2 = axes[1]\n",
    "avg_times = [np.mean(times[m]) for m in ['A', 'B', 'C']]\n",
    "std_times = [np.std(times[m]) for m in ['A', 'B', 'C']]\n",
    "bars = ax2.bar(['Model A', 'Model B', 'Model C'], avg_times, yerr=std_times,\n",
    "               color=[MODEL_COLORS[m] for m in ['A', 'B', 'C']], capsize=5, alpha=0.8)\n",
    "\n",
    "for bar, t in zip(bars, avg_times):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, f'{t:.3f}s',\n",
    "             ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "ax2.set_ylabel('Average Time (seconds)', fontsize=11)\n",
    "ax2.set_title('(B) Average Computation Time', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 可视化: 综合性能雷达图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算各模型的综合指标\n",
    "metrics_all = {}\n",
    "for model in ['A', 'B', 'C']:\n",
    "    scores = [r['score'] for r in results[model]]\n",
    "    metrics_all[model] = {\n",
    "        'Accuracy': np.mean(scores),\n",
    "        'Stability': 1 - np.std(scores),\n",
    "        'Speed': 1 - min(1, np.mean(times[model]) / max(np.mean(times['A']), np.mean(times['B']), np.mean(times['C']))),\n",
    "        'Robustness': len([s for s in scores if s > 0.5]) / len(scores),\n",
    "        'Interpretability': {'A': 0.9, 'B': 0.7, 'C': 0.6}[model]\n",
    "    }\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
    "\n",
    "categories = list(metrics_all['A'].keys())\n",
    "angles = [n / float(len(categories)) * 2 * np.pi for n in range(len(categories))]\n",
    "angles += angles[:1]\n",
    "\n",
    "for model in ['A', 'B', 'C']:\n",
    "    values = list(metrics_all[model].values())\n",
    "    values += values[:1]\n",
    "    ax.plot(angles, values, 'o-', linewidth=2, color=MODEL_COLORS[model], label=f'Model {model}')\n",
    "    ax.fill(angles, values, alpha=0.15, color=MODEL_COLORS[model])\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, fontsize=11)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "ax.set_title('Three Models: Comprehensive Performance Comparison', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 可视化: 性能分布对比（小提琴图）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "data_for_violin = []\n",
    "labels_for_violin = []\n",
    "for model in ['A', 'B', 'C']:\n",
    "    scores = [r['score'] for r in results[model]]\n",
    "    data_for_violin.append(scores)\n",
    "    labels_for_violin.append(f'Model {model}')\n",
    "\n",
    "parts = ax.violinplot(data_for_violin, positions=[1, 2, 3], showmeans=True, showmedians=True)\n",
    "\n",
    "for i, (pc, model) in enumerate(zip(parts['bodies'], ['A', 'B', 'C'])):\n",
    "    pc.set_facecolor(MODEL_COLORS[model])\n",
    "    pc.set_alpha(0.7)\n",
    "\n",
    "ax.set_xticks([1, 2, 3])\n",
    "ax.set_xticklabels(['Model A\\n(Linear Programming)', 'Model B\\n(Bayesian MCMC)', 'Model C\\n(Genetic Algorithm)'])\n",
    "ax.set_ylabel('Performance Score', fontsize=11)\n",
    "ax.set_title('Performance Distribution Comparison (Violin Plot)', fontsize=13, fontweight='bold')\n",
    "\n",
    "# 添加统计信息\n",
    "for i, model in enumerate(['A', 'B', 'C']):\n",
    "    scores = [r['score'] for r in results[model]]\n",
    "    ax.text(i+1, max(scores)+0.05, f'Mean: {np.mean(scores):.2f}\\nStd: {np.std(scores):.2f}',\n",
    "            ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 可视化: 单周对比热力图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建对比矩阵\n",
    "comparison_data = []\n",
    "for i in range(min(20, len(results['A']))):\n",
    "    comparison_data.append([\n",
    "        results['A'][i]['score'],\n",
    "        results['B'][i]['score'],\n",
    "        results['C'][i]['score']\n",
    "    ])\n",
    "\n",
    "comparison_matrix = np.array(comparison_data)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 12))\n",
    "\n",
    "sns.heatmap(comparison_matrix, cmap='RdYlGn', vmin=0, vmax=1,\n",
    "            xticklabels=['Model A', 'Model B', 'Model C'],\n",
    "            yticklabels=[f\"S{results['A'][i]['season']}W{results['A'][i]['week']}\" for i in range(len(comparison_data))],\n",
    "            annot=True, fmt='.2f', cbar_kws={'label': 'Score'}, ax=ax)\n",
    "\n",
    "ax.set_xlabel('Model', fontsize=11)\n",
    "ax.set_ylabel('Season-Week', fontsize=11)\n",
    "ax.set_title('Performance Comparison Heatmap', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 汇总统计表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data = []\n",
    "for model in ['A', 'B', 'C']:\n",
    "    scores = [r['score'] for r in results[model]]\n",
    "    summary_data.append({\n",
    "        'Model': f'Model {model}',\n",
    "        'Method': {'A': 'Linear Programming', 'B': 'Bayesian MCMC', 'C': 'Genetic Algorithm'}[model],\n",
    "        'Mean Score': f'{np.mean(scores):.3f}',\n",
    "        'Std': f'{np.std(scores):.3f}',\n",
    "        'Min': f'{np.min(scores):.3f}',\n",
    "        'Max': f'{np.max(scores):.3f}',\n",
    "        'Avg Time (s)': f'{np.mean(times[model]):.4f}',\n",
    "        'Success Rate': f'{len([s for s in scores if s > 0]) / len(scores):.1%}'\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print('=' * 80)\n",
    "print('三种方案综合对比汇总')\n",
    "print('=' * 80)\n",
    "print(summary_df.to_string(index=False))\n",
    "print('=' * 80)\n",
    "\n",
    "# 保存为Excel\n",
    "print('汇总表已保存: figures/三方案_汇总统计.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 学术化统计分析\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 方差分析 (ANOVA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway, kruskal, levene\n",
    "\n",
    "scores_a = [r['score'] for r in results['A']]\n",
    "scores_b = [r['score'] for r in results['B']]\n",
    "scores_c = [r['score'] for r in results['C']]\n",
    "\n",
    "print('=' * 70)\n",
    "print('Table 13: One-Way ANOVA - Model Performance Comparison')\n",
    "print('=' * 70)\n",
    "print('H0: All models have equal mean performance')\n",
    "print('H1: At least one model differs significantly')\n",
    "print('-' * 70)\n",
    "\n",
    "# Levene's test for homogeneity of variances\n",
    "stat, p = levene(scores_a, scores_b, scores_c)\n",
    "print(f'Levene\\'s Test (Homogeneity): F={stat:.4f}, p={p:.4f}')\n",
    "print(f'  Interpretation: Variances are {\"equal\" if p>0.05 else \"unequal\"}')\n",
    "\n",
    "# One-way ANOVA\n",
    "f_stat, p_value = f_oneway(scores_a, scores_b, scores_c)\n",
    "print(f'\\nOne-Way ANOVA: F={f_stat:.4f}, p={p_value:.4f}')\n",
    "\n",
    "# Kruskal-Wallis (non-parametric alternative)\n",
    "h_stat, p_kw = kruskal(scores_a, scores_b, scores_c)\n",
    "print(f'Kruskal-Wallis H-test: H={h_stat:.4f}, p={p_kw:.4f}')\n",
    "\n",
    "# Effect size (eta-squared)\n",
    "all_scores = scores_a + scores_b + scores_c\n",
    "grand_mean = np.mean(all_scores)\n",
    "ss_between = len(scores_a)*(np.mean(scores_a)-grand_mean)**2 + len(scores_b)*(np.mean(scores_b)-grand_mean)**2 + len(scores_c)*(np.mean(scores_c)-grand_mean)**2\n",
    "ss_total = np.sum((np.array(all_scores) - grand_mean)**2)\n",
    "eta_squared = ss_between / ss_total if ss_total > 0 else 0\n",
    "print(f'\\nEffect Size (eta-squared): {eta_squared:.4f}')\n",
    "print(f'  Interpretation: {\"Large\" if eta_squared>0.14 else \"Medium\" if eta_squared>0.06 else \"Small\"} effect')\n",
    "print('=' * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 事后检验 (Post-hoc Tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "from itertools import combinations\n",
    "\n",
    "print('=' * 70)\n",
    "print('Table 14: Pairwise Comparisons (Bonferroni Corrected)')\n",
    "print('=' * 70)\n",
    "\n",
    "models = {'A': scores_a, 'B': scores_b, 'C': scores_c}\n",
    "n_comparisons = 3\n",
    "alpha_corrected = 0.05 / n_comparisons\n",
    "\n",
    "print(f'Bonferroni corrected alpha: {alpha_corrected:.4f}')\n",
    "print('-' * 70)\n",
    "print(f'{\"Comparison\":<20} {\"t-stat\":<12} {\"p-value\":<12} {\"Cohen d\":<12} {\"Significant\":<12}')\n",
    "print('-' * 70)\n",
    "\n",
    "for (m1, s1), (m2, s2) in combinations(models.items(), 2):\n",
    "    t_stat, p_val = ttest_ind(s1, s2)\n",
    "    pooled_std = np.sqrt(((len(s1)-1)*np.var(s1) + (len(s2)-1)*np.var(s2)) / (len(s1)+len(s2)-2))\n",
    "    cohens_d = (np.mean(s1) - np.mean(s2)) / pooled_std if pooled_std > 0 else 0\n",
    "    sig = 'Yes' if p_val < alpha_corrected else 'No'\n",
    "    print(f'{m1} vs {m2:<15} {t_stat:<12.4f} {p_val:<12.4f} {cohens_d:<12.4f} {sig:<12}')\n",
    "\n",
    "print('=' * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. 置信区间与效应量可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import sem, t\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# (A) 95% CI Forest Plot\n",
    "ax1 = axes[0]\n",
    "model_names = ['Model A\\n(LP)', 'Model B\\n(MCMC)', 'Model C\\n(GA)']\n",
    "means = [np.mean(scores_a), np.mean(scores_b), np.mean(scores_c)]\n",
    "sems = [sem(scores_a), sem(scores_b), sem(scores_c)]\n",
    "ns = [len(scores_a), len(scores_b), len(scores_c)]\n",
    "\n",
    "# 95% CI\n",
    "cis = [t.ppf(0.975, n-1) * se for se, n in zip(sems, ns)]\n",
    "\n",
    "y_pos = np.arange(3)\n",
    "ax1.errorbar(means, y_pos, xerr=cis, fmt='o', color='black', capsize=5, markersize=8)\n",
    "ax1.axvline(x=np.mean(all_scores), color='red', linestyle='--', label='Grand Mean')\n",
    "\n",
    "for i, (m, ci) in enumerate(zip(means, cis)):\n",
    "    ax1.fill_betweenx([i-0.1, i+0.1], m-ci, m+ci, alpha=0.3, color=list(MODEL_COLORS.values())[i])\n",
    "\n",
    "ax1.set_yticks(y_pos)\n",
    "ax1.set_yticklabels(model_names)\n",
    "ax1.set_xlabel('Performance Score', fontsize=11)\n",
    "ax1.set_title('(A) 95% Confidence Intervals (Forest Plot)', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "\n",
    "# (B) Effect Size Comparison\n",
    "ax2 = axes[1]\n",
    "comparisons = ['A vs B', 'A vs C', 'B vs C']\n",
    "effect_sizes = []\n",
    "for (s1, s2) in [(scores_a, scores_b), (scores_a, scores_c), (scores_b, scores_c)]:\n",
    "    pooled_std = np.sqrt(((len(s1)-1)*np.var(s1) + (len(s2)-1)*np.var(s2)) / (len(s1)+len(s2)-2))\n",
    "    d = abs(np.mean(s1) - np.mean(s2)) / pooled_std if pooled_std > 0 else 0\n",
    "    effect_sizes.append(d)\n",
    "\n",
    "colors = ['green' if d<0.2 else 'orange' if d<0.8 else 'red' for d in effect_sizes]\n",
    "bars = ax2.barh(comparisons, effect_sizes, color=colors, alpha=0.7)\n",
    "ax2.axvline(x=0.2, color='green', linestyle=':', label='Small (0.2)')\n",
    "ax2.axvline(x=0.5, color='orange', linestyle=':', label='Medium (0.5)')\n",
    "ax2.axvline(x=0.8, color='red', linestyle=':', label='Large (0.8)')\n",
    "ax2.set_xlabel('Cohen\\'s d (Effect Size)', fontsize=11)\n",
    "ax2.set_title('(B) Pairwise Effect Sizes', fontsize=12, fontweight='bold')\n",
    "ax2.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. 模型选择准则 (AIC/BIC近似)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于性能和复杂度的模型选择\n",
    "print('=' * 70)\n",
    "print('Table 15: Model Selection Criteria')\n",
    "print('=' * 70)\n",
    "\n",
    "# 模型参数数量估计\n",
    "model_params = {'A': 2, 'B': 3, 'C': 4}  # epsilon, alpha, pop_size, mutation_rate, etc.\n",
    "\n",
    "print(f'{\"Model\":<15} {\"Mean Score\":<12} {\"Params\":<10} {\"AIC-like\":<12} {\"BIC-like\":<12}')\n",
    "print('-' * 70)\n",
    "\n",
    "for model in ['A', 'B', 'C']:\n",
    "    scores = [r['score'] for r in results[model]]\n",
    "    n = len(scores)\n",
    "    k = model_params[model]\n",
    "    \n",
    "    # 使用负对数似然近似 (1 - score作为loss)\n",
    "    neg_ll = -np.sum(np.log(np.array(scores) + 0.01))\n",
    "    \n",
    "    aic = 2*k + 2*neg_ll\n",
    "    bic = k*np.log(n) + 2*neg_ll\n",
    "    \n",
    "    print(f'Model {model:<10} {np.mean(scores):<12.4f} {k:<10} {aic:<12.2f} {bic:<12.2f}')\n",
    "\n",
    "print('=' * 70)\n",
    "print('Note: Lower AIC/BIC indicates better model (balancing fit and complexity)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. 结论与建议"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 80)\n",
    "print('结论与建议')\n",
    "print('=' * 80)\n",
    "\n",
    "# 找出最佳模型\n",
    "avg_scores = {m: np.mean([r['score'] for r in results[m]]) for m in ['A', 'B', 'C']}\n",
    "best_model = max(avg_scores, key=avg_scores.get)\n",
    "\n",
    "print(f'''\n",
    "1. 性能对比:\n",
    "   - Model A (LP): 平均得分 {avg_scores[\"A\"]:.3f}\n",
    "   - Model B (MCMC): 平均得分 {avg_scores[\"B\"]:.3f}\n",
    "   - Model C (GA): 平均得分 {avg_scores[\"C\"]:.3f}\n",
    "   - 最佳模型: Model {best_model}\n",
    "\n",
    "2. 计算效率:\n",
    "   - Model A: {np.mean(times[\"A\"]):.4f}s (最快)\n",
    "   - Model B: {np.mean(times[\"B\"]):.4f}s\n",
    "   - Model C: {np.mean(times[\"C\"]):.4f}s\n",
    "\n",
    "3. 适用场景:\n",
    "   - Model A: 适合需要精确解和数学严谨性的场景\n",
    "   - Model B: 适合需要不确定性量化的场景\n",
    "   - Model C: 适合复杂约束和大规模问题\n",
    "\n",
    "4. 建议:\n",
    "   - 论文中可采用Model {best_model}作为主要方法\n",
    "   - 结合多种方法进行交叉验证\n",
    "   - 根据具体问题特点选择合适的模型\n",
    "''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}