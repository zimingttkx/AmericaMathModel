# 数据处理 AI Prompt 模板（详细版）

> **使用说明**: 本文件包含数据清洗、预处理、特征工程等数据处理环节的详细Prompt模板。

---

## 🔧 场景1: 数据读取与基础检查

### 读取CSV文件完整模板

```
请帮我读取CSV文件并进行基础检查。

【文件信息】
文件路径：【提供完整路径或文件名】
编码格式：utf-8 / gbk / latin-1 / 自动检测
分隔符：逗号(,) / 分号(;) / 制表符(\t)
小数点：点(.) / 逗号(,)

【基础检查要求】
1. 显示数据基本信息
   - 数据形状（行数、列数）
   - 列名和数据类型
   - 内存使用情况
   
2. 显示前10行和后10行数据
   - 使用head()和tail()
   - 确认数据读取正确

3. 数据类型检查
   - 显示每列的数据类型
   - 识别可能的类型转换需求
   - 检查是否有object类型应该是数值

4. 缺失值检查
   - 统计每列的缺失值数量
   - 计算缺失值比例
   - 生成缺失值报告

5. 重复值检查
   - 检查是否有完全重复的行
   - 统计重复行数量
   - 显示重复行示例

6. 基础统计
   - 数值列：describe()统计
   - 分类列：unique()统计
   - 显示分布情况

【代码要求】
使用pandas库
添加详细的中文注释
包含错误处理（文件不存在、编码错误）
显示友好的输出格式

【输出示例格式】
=== 数据基本信息 ===
形状: (1000, 20)
列名: ['id', 'name', 'age', ...]
内存使用: 15.6 MB

=== 前5行数据 ===
...

=== 缺失值统计 ===
列名        缺失数    缺失比例
id          0        0.0%
name        5        0.5%
...

【期望输出】
提供完整的Python代码，包括：
- 导入语句
- 文件读取
- 各种检查
- 格式化输出
```

### 读取Excel文件完整模板

```
请读取Excel文件。

【文件信息】
文件路径：【完整路径】
工作表名称：【Sheet1 / 具体名称】
表头行：第几行（默认0）
数据起始行：第几行

【读取要求】
1. 读取指定工作表
2. 处理多级表头（如果有）
3. 跳过不需要的行
4. 指定列名

【检查要求】
同CSV文件的检查要求

【特殊处理】
- 合并多个工作表（如果需要）
- 处理合并单元格
- 处理公式单元格（只读值）

【代码要求】
使用pandas的read_excel
openpyxl或xlrd引擎
添加错误处理
```

---

## 🧹 场景2: 缺失值处理

### 缺失值分析与处理完整模板

```
请帮我分析和处理数据中的缺失值。

【数据信息】
数据框名称：【df】
总行数：【行数】
总列数：【列数】

【缺失值分析】
1. 详细的缺失值统计
   - 每列的缺失数量
   - 缺失比例
   - 缺失模式分析（MCAR/MAR/MNAR）
   
2. 可视化缺失值
   - 缺失值热力图
   - 缺失值条形图
   - 使用missingno库（如果可用）

3. 缺失值相关性
   - 某列缺失是否与其他列相关
   - 缺失值模式的聚类分析

【处理策略】

列删除策略：
  - 删除缺失比例超过【50%】的列
  - 给出删除原因和建议

行删除策略：
  - 如果缺失行比例小于【5%】，考虑删除
  - 关键列缺失则必须删除
  - 给出删除前后的数据对比

填充策略（逐列制定）：
  
数值列填充：
  - 均值填充：适用于正态分布数据
    - 列名：【列出列名】
    - 方法：mean
    
  - 中位数填充：适用于有异常值的数据
    - 列名：【列出列名】
    - 方法：median
    
  - 众数填充：适用于分类或离散数值
    - 列名：【列出列名】
    - 方法：mode
    
  - 前向填充：适用于时间序列
    - 列名：【列出列名】
    - 方法：ffill
    
  - 后向填充：适用于时间序列
    - 列名：【列出列名】
    - 方法：bfill
    
  - 插值填充：适用于连续数值
    - 列名：【列出列名】
    - 方法：linear/多项式/样条
    - 方法：interpolate(method='linear')

分类列填充：
  - 众数填充：最常用
    - 列名：【列出列名】
    
  - 新类别填充：
    - 列名：【列出列名】
    - 新类别值：'Unknown' / 'Missing'
    
  - 基于其他列预测填充：
    - 使用机器学习模型预测缺失值
    - 列名：【重要列名】

【高级填充方法】
1. KNN填充
   - 使用sklearn.impute.KNNImputer
   - k值：【3-5】
   
2. 迭代填充（MICE）
   - 使用sklearn的IterativeImputer
   - 最多迭代：【10】次
   
3. 随机森林填充
   - 使用sklearn.ensemble.RandomForestRegressor
   - 基于其他列预测

【填充后的验证】
1. 再次检查缺失值（确保无缺失）
2. 对比填充前后的数据分布
3. 检查填充是否引入偏差
4. 显示填充总结报告

【代码要求】
使用pandas、numpy
可选：missingno、sklearn
详细的步骤注释
每步都有验证
生成处理报告

【输出格式】
=== 缺失值处理报告 ===
原始缺失情况:
...
处理策略:
...
处理后检查:
...
填充建议:
...
```

### 实际使用示例

```
请处理Titanic数据集的缺失值：

【数据】
df包含以下列：Age, Fare, Cabin, Embarked

【缺失情况】
Age: 20%缺失
Cabin: 77%缺失  
Embarked: 0.2%缺失

【处理策略】
- Cabin: 缺失太多，删除该列
- Age: 用中位数填充（考虑Sex和Pclass）
- Embarked: 用众数填充

【要求】
- 显示处理前后对比
- 验证填充合理性
- 生成完整代码

请提供详细代码和解释。
```

---

## 📊 场景3: 异常值检测与处理

### 异常值检测完整模板

```
请帮我检测和处理数据中的异常值。

【数据信息】
数据框：【df】
需要检测的列：【数值列列表】

【检测方法】

方法1：统计学方法（3σ原则）
  - 适用：正态分布数据
  - 阈值：均值 ± 3×标准差
  - 列名：【列出列名】
  
  实现：
  ```python
  mean = df[col].mean()
  std = df[col].std()
  lower = mean - 3 * std
  upper = mean + 3 * std
  outliers = df[(df[col] < lower) | (df[col] > upper)]
  ```

方法2：箱线图方法（IQR）
  - 适用：任何分布
  - 阈值：Q1 - 1.5×IQR 或 Q3 + 1.5×IQR
  - 列名：【列出列名】
  
  实现：
  ```python
  Q1 = df[col].quantile(0.25)
  Q3 = df[col].quantile(0.75)
  IQR = Q3 - Q1
  lower = Q1 - 1.5 * IQR
  upper = Q3 + 1.5 * IQR
  outliers = df[(df[col] < lower) | (df[col] > upper)]
  ```

方法3：Z-score方法
  - 适用：正态分布数据
  - 阈值：|Z| > 3
  - 列名：【列出列名】

方法4：DBSCAN聚类
  - 适用：多维异常检测
  - 参数：eps=0.5, min_samples=5
  - 标记离群点

【可视化检测】
1. 箱线图
   - 每列的箱线图
   - 标记异常点
   
2. 散点图
   - 两变量关系散点图
   - 标记远离中心的点
   
3. QQ图
   - 检查正态性
   - 识别偏离正态的点
   
4. Z-score图
   - 显示所有点的Z-score
   - 标记超出阈值的点

【异常值统计】
1. 每列的异常值数量
2. 异常值比例
3. 异常值索引列表
4. 异常值的数值范围

【处理策略】

策略1：删除异常值
  - 适用：确认是错误数据
  - 列名：【列名】
  - 方法：直接删除或条件过滤
  
  实现：
  ```python
  df_clean = df[(df[col] >= lower) & (df[col] <= upper)]
  ```

策略2：替换为边界值
  - 适用：保留数据但限制影响
  - 列名：【列名】
  - 替换为：lower或upper边界
  
  实现：
  ```python
  df[col] = df[col].clip(lower=lower, upper=upper)
  ```

策略3：替换为中心值
  - 适用：减少异常值影响
  - 列名：【列名】
  - 替换为：mean/median

策略4：分箱处理
  - 适用：离散化数据
  - 列名：【列名】
  - 分箱数：【5或10】

策略5：保持不变
  - 适用：可能是真实数据
  - 列名：【列名】
  - 原因：【说明原因】

【处理后的验证】
1. 异常值处理后的统计对比
2. 数据分布变化（直方图对比）
3. 模型性能对比（如果有）
4. 业务合理性检查

【代码要求】
使用pandas、numpy、matplotlib、seaborn
可选：scipy.stats, sklearn
详细的检测步骤
可视化展示
生成检测报告

【输出格式】
=== 异常值检测报告 ===
列名: 【列名】
检测方法: 3σ原则
异常值数量: 15
异常值比例: 1.5%
异常值索引: [...]
处理策略: 删除
处理后: ...
```

### 实际使用示例

```
请检测房价数据中的异常值：

【数据】
列名：Price, Area, Rooms
Price: 价格（万元）
Area: 面积（平方米）
Rooms: 房间数

【检测】
使用箱线图方法
阈值为：1.5×IQR
可视化展示

【处理】
Price: 删除异常高值（可能是录入错误）
Area: 限制在10-500平方米之间
Rooms: 限制在1-10之间

【要求】
显示处理前后对比
生成完整代码
详细注释
```

---

## 🔢 场景4: 数据类型转换

### 数据类型转换完整模板

```
请帮我进行数据类型转换和编码。

【数据信息】
数据框：【df】
原始类型统计：【显示各列类型】

【数值型转换】

转换为数值：
  列名：【列表】
  原类型：object
  目标：float64
  
  处理步骤：
  1. 移除非数字字符（如：$, ,）
  2. 处理千分位分隔符
  3. 转换为float类型
  4. 验证转换结果
  
  代码：
  ```python
  df['列名'] = df['列名'].str.replace('$', '').str.replace(',', '').astype(float)
  ```

转换为整数：
  列名：【列表】
  原类型：float或object
  目标：int64
  
  注意：检查是否有小数

转换为日期时间：
  列名：【列表】
  原格式：字符串
  目标格式：datetime64[ns]
  格式字符串：%Y-%m-%d（或其他）
  
  提取特征：
  - 年份
  - 月份
  - 日
  - 星期
  - 小时（如果有）

【分类编码】

Label Encoding（有序分类）：
  列名：【列名】
  类别：低、中、高（有顺序）
  
  使用：sklearn.preprocessing.LabelEncoder
  保存编码映射关系

One-Hot Encoding（无序分类）：
  列名：【列名】
  类别：红色、蓝色、绿色（无顺序）
  
  使用：pd.get_dummies或sklearn OneHotEncoder
  删除第一列（避免多重共线性）
  列名命名：原类别名（如：Color_Red, Color_Blue）

Binary Encoding：
  列名：【列名】
  适用：类别较多（>10个）
  
  使用：category_encoders.BinaryEncoder

Frequency Encoding：
  列名：【列名】
  方法：用频次替换类别
  适用于：高基数分类

Target Encoding：
  列名：【列名】
  目标变量：【y】
  方法：用目标均值替换类别
  注意：防止过拟合

【验证要求】
1. 显示转换前后对比
2. 检查是否有转换失败的值
3. 显示新的数据类型
4. 验证编码的完整性

【代码要求】
使用pandas、sklearn
详细的步骤注释
处理异常值
验证结果
生成转换报告
```

---

## 📅 场景5: 时间序列数据处理

### 时间序列处理完整模板

```
请处理时间序列数据。

【数据信息】
时间列：【列名】
当前格式：字符串/时间戳
目标格式：datetime64[ns]

【转换步骤】
1. 转换为datetime类型
   ```python
   df['时间列'] = pd.to_datetime(df['时间列'])
   ```

2. 设置为索引（如果需要）
   ```python
   df = df.set_index('时间列')
   ```

3. 检查时间序列质量
   - 是否有缺失时间点
   - 时间间隔是否一致
   - 时间范围

【特征提取】
从时间列提取：
  - 年份：year
  - 月份：month
  - 日：day
  - 星期：dayofweek (0=周一, 6=周日)
  - 小时：hour
  - 季节：season
  - 是否周末：is_weekend
  - 月初：is_month_start
  - 月末：is_month_end

【重采样】
重采样频率：
  - 按天：'D'或'daily'
  - 按周：'W'
  - 按月：'M'
  - 按季度：'Q'
  - 按年：'Y'

聚合方法：
  - 求和：resample().sum()
  - 均值：resample().mean()
  - 最大值：resample().max()
  - 最小值：resample().min()
  - 计数：resample().count()

【滚动窗口】
窗口大小：【7天/30天】
  - 滚动均值：rolling(window).mean()
  - 滚动标准差：rolling(window).std()
  - 滚动最大值：rolling(window).max()
  - 滚动最小值：rolling(window).min()

【差分和滞后】
1. 差分
   - 一阶差分：diff()
   - 二阶差分：diff().diff()
   - 季节性差分：diff(periods=12)

2. 滞后特征
   - 滞后1期：shift(1)
   - 滞后7期：shift(7)
   - 滞后30期：shift(30)

【缺失值处理】
时间序列特殊处理：
  - 前向填充：ffill()
  - 后向填充：bfill()
  - 线性插值：interpolate(method='linear')

【代码要求】
使用pandas
设置时间索引
详细的特征提取
可视化时间序列
生成处理报告
```

---

## 🔄 场景6: 数据合并与拼接

### 数据合并完整模板

```
请帮我合并多个数据框。

【合并方式1：横向合并（merge）】

左表：【df1】
右表：【df2】或【df3】
合并键（键）：【列名或多个列名】
合并方式：
  - inner：内连接（只保留匹配）
  - left：左连接（保留左表所有）
  - right：右连接（保留右表所有）
  - outer：外连接（保留所有）

后缀：
  - 左表后缀：_left
  - 右表后缀：_right

实现代码：
```python
merged = pd.merge(df1, df2, 
                 on='键列',
                 how='inner',
                 suffixes=('_left', '_right'))
```

【合并方式2：纵向拼接（concat）】

数据框列表：[df1, df2, df3, ...]
拼接方向：axis=0（纵向）
是否忽略索引：ignore_index=True
是否排序：sort=True

实现代码：
```python
combined = pd.concat([df1, df2, df3], 
                      axis=0, 
                      ignore_index=True,
                      sort=True)
```

【验证要求】
1. 显示合并前后形状
2. 检查键的唯一性
3. 检查缺失值
4. 验证合并的正确性
```

---

## ⚡ 场景7: 数据筛选与采样

### 数据筛选完整模板

```
请帮我筛选数据。

【条件筛选】
单条件：
  - 列名：【列名】
  - 操作符：>, <, ==, !=, >=, <=, in, not in
  - 值：【具体值】

多条件（AND）：
  - 条件1：列1 > 值1
  - 条件2：列2 == 值2
  - 连接：&

多条件（OR）：
  - 条件1：列1 == 值1
  - 条件2：列2 == 值2
  - 连接：|

字符串筛选：
  - 包含：contains()
  - 开头：startswith()
  - 结尾：endswith()
  - 正则：match()

【随机抽样】
抽样比例：0.7或0.8
抽样数量：1000
随机种子：42（可重复）
是否放回：False

分层抽样：
  分层依据：【列名】
  各层比例：保持或自定义

【代码要求】
使用pandas的条件筛选
loc或iloc索引
sample函数
设置随机种子
验证抽样结果
```

---

## 💡 常用代码片段

### 数据质量检查报告
```
请生成完整的数据质量报告：

【报告内容】
1. 数据概览（形状、类型、内存）
2. 缺失值详细分析
3. 异常值检测
4. 重复值检查
5. 数据类型合理性
6. 统计摘要
7. 可视化展示

【输出格式】
结构化报告
包含表格和图表
保存为文本文件或打印
```

### 数据清洗pipeline
```
请创建数据清洗流程：

【清洗步骤】
1. 读取数据
2. 处理缺失值
3. 处理异常值
4. 类型转换
5. 去除重复
6. 数据验证

【要求】
创建函数形式
每个步骤可配置
中间结果验证
生成清洗日志
```

---

**文件结束**

本文件涵盖了数据处理的所有主要场景，每个模板都包含详细的参数说明和代码要求，确保AI能生成高质量的数据处理代码。
