# 特征工程 AI Prompt 模板（详细版）

> **使用说明**: 本文件包含特征选择、特征变换、降维等特征工程环节的详细Prompt模板。

---

## 🎯 场景1: 特征选择方法

### 方差阈值选择完整模板

```
请使用方差阈值法选择特征。

【数据信息】
特征矩阵：【X】
特征名称：【列出所有特征名】
数据类型：数值型

【参数设置】
阈值：0.01或0.1
解释：删除方差小于阈值的特征

【实现步骤】
1. 计算每个特征的方差
2. 比较方差与阈值
3. 识别低方差特征
4. 删除低方差特征
5. 输出被删除的特征列表
6. 输出保留的特征列表

【代码要求】
使用sklearn.feature_selection.VarianceThreshold
实现：
```python
from sklearn.feature_selection import VarianceThreshold

selector = VarianceThreshold(threshold=0.01)
X_selected = selector.fit_transform(X)

# 获取被保留的特征索引
selected_indices = selector.get_support(indices=True)
selected_features = [feature_names[i] for i in selected_indices]
```

【验证要求】
1. 对比选择前后的特征数量
2. 显示被删除特征的方差值
3. 可视化方差分布
4. 评估对模型性能的影响

【输出报告】
=== 方差阈值特征选择报告 ===
原始特征数: 【数量】
阈值: 【值】
删除特征数: 【数量】
保留特征数: 【数量】
删除特征列表: 【列表】
保留特征列表: 【列表】
```

### 相关性特征选择完整模板

```
请使用相关性方法选择特征。

【数据信息】
特征矩阵：【X】
目标变量：【y】（有监督）
相关系数阈值：0.8或0.9

【方法说明】
删除高相关特征：
  - 计算特征间的相关系数
  - 如果两个特征相关系数 > 阈值
  - 删除与目标变量相关性较低的一个

【实现步骤】
1. 计算相关系数矩阵
2. 识别高相关特征对
3. 对每对特征：
   - 计算与目标变量的相关性
   - 删除相关性较低的特征
4. 确保不重复删除

【代码要求】
使用pandas和numpy
相关系数：pearson或spearman
可视化：热力图
代码逻辑严谨

【可视化要求】
1. 相关性矩阵热力图（删除前）
2. 高相关特征对标记
3. 删除后的相关性热力图
4. 特征重要性对比

【输出报告】
=== 相关性特征选择报告 ===
原始特征数: 【数量】
相关系数阈值: 【值】
删除的高相关对:
  - 特征A & 特征B: 相关系数【值】
  - 特征C & 特征D: 相关系数【值】
删除的特征: 【列表】
保留的特征: 【列表】
```

### 模型特征重要性选择完整模板

```
请使用模型特征重要性选择特征。

【数据信息】
特征矩阵：【X_train, X_test】
目标变量：【y_train, y_test】

【方法1：随机森林特征重要性】
模型参数：
  - n_estimators: 100
  - max_depth: None
  - random_state: 42

实现步骤：
1. 训练随机森林模型
2. 获取特征重要性
3. 可视化特征重要性
4. 选择Top K个特征

代码：
```python
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

importances = rf.feature_importances_
indices = np.argsort(importances)[::-1]
```

【方法2：XGBoost特征重要性】
模型参数：
  - learning_rate: 0.1
  - max_depth: 6
  - n_estimators: 100

【方法3：LASSO特征选择】
模型：LassoCV
正则化强度：通过交叉验证选择
选择非零系数的特征

【选择Top K特征】
K值：10或根据累积重要性确定（如累积95%）
选择策略：
  - 累积重要性阈值
  - 固定数量Top K

【验证要求】
1. 特征重要性排序
2. 可视化特征重要性（水平柱状图）
3. 累积重要性曲线
4. 对比选择前后的模型性能
5. 特征数量 vs 性能曲线

【输出报告】
=== 模型特征重要性选择报告 ===
模型类型: 随机森林
特征总数: 【总数】
选择的Top K: 【K值】
特征重要性排名:
  1. 【特征名】: 重要性【值】
  2. 【特征名】: 重要性【值】
  ...

【推荐特征】
Top 10特征:
  【列出10个特征】

【性能对比】
使用所有特征: 准确率【值】
使用Top 10特征: 准确率【值】
性能变化: 【提升或下降】
```

### RFE递归特征消除完整模板

```
请使用递归特征消除(RFE)选择特征。

【数据信息】
特征矩阵：【X】
目标变量：【y】

【参数设置】
基础模型：LogisticRegression或RandomForest
特征数量目标：10或由交叉验证确定
步长：每次消除1个或几个特征
交叉验证：5折

【实现步骤】
1. 创建RFE对象
2. 使用交叉验证确定最优特征数
3. 执行RFE
4. 获取选定的特征
5. 验证特征选择效果

代码：
```python
from sklearn.feature_selection import RFE
from sklearn.model_selection import cross_val_score

# 创建基础模型
estimator = LogisticRegression(random_state=42)

# 创建RFE对象
selector = RFE(estimator, n_features_to_select=10, step=1)

# 执行RFE
selector = selector.fit(X, y)

# 获取选定的特征
selected_features = [feature_names[i] for i in range(len(feature_names)) 
                     if selector.support_[i]]
```

【验证要求】
1. 特征排名
2. 特征选择过程可视化
3. 交叉验证性能曲线
4. 最终特征列表
```

---

## 📊 场景2: 特征变换与创建

### 多项式特征完整模板

```
请创建多项式特征。

【数据信息】
原始特征：【X】
特征名称：【列出特征名】
多项式阶数：2或3

【变换内容】
交互项：是否创建 x1*x2
平方项：是否创建 x1^2
立方项：是否创建 x1^3

【实现步骤】
1. 使用PolynomialFeatures
2. 拟合变换
3. 显示新特征名称
4. 对比变换前后

代码：
```python
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X)

# 获取特征名称
feature_names_poly = poly.get_feature_names_out(input_features=feature_names)
```

【验证要求】
1. 显示新特征数量
2. 列出新特征名称
3. 检查多重共线性
4. 评估模型性能变化
```

### 对数变换完整模板

```
请进行对数变换。

【适用情况】
特征：【特征名】
分布：右偏分布（长尾）
原因：降低偏度，接近正态分布

【变换方法】
方法1：自然对数 ln(x)
  - 适用于：正值
  - 代码：np.log(x)

方法2：以10为底的对数 log10(x)
  - 适用于：正值
  - 代码：np.log10(x)

方法3：log1p(x) = log(1+x)
  - 适用于：包含0或接近0的值
  - 代码：np.log1p(x)

【实现步骤】
1. 检查数据范围（是否有负值或0）
2. 选择合适的对数方法
3. 应用变换
4. 验证变换效果

【验证要求】
1. 变换前后分布对比（直方图或QQ图）
2. 偏度和峰度对比
3. 正态性检验
4. 模型性能对比
```

### Box-Cox变换完整模板

```
请进行Box-Cox变换。

【适用情况】
数据：正值（所有值>0）
目的：稳定方差，正态化

【实现步骤】
1. 检查数据范围
2. 使用scipy.stats.boxcox
3. 确定最优lambda参数
4. 应用变换
5. 验证效果

代码：
```python
from scipy.stats import boxcox

# 执行Box-Cox变换
transformed_data, lambda_param = boxcox(original_data)

# lambda解释
# lambda = 1: 不变换
# lambda = 0: 对数变换
# lambda = 0.5: 平方根变换
# lambda = -1: 倒数变换
```

【验证要求】
1. 最优lambda值
2. 变换前后分布对比
3. 正态性检验
4. QQ图对比
```

### 标准化与归一化完整模板

```
请对数据进行标准化/归一化。

【方法1：Z-score标准化】
目的：均值0，方差1
公式：(x - mean) / std
适用：大多数机器学习算法

代码：
```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 查看均值和方差
print("均值:", scaler.mean_)
print("方差:", scaler.var_)
```

【方法2：Min-Max归一化】
目的：范围[0, 1]或[-1, 1]
公式：(x - min) / (max - min)
适用：神经网络、图像处理

代码：
```python
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler(feature_range=(0, 1))
X_normalized = scaler.fit_transform(X)
```

【方法3：Robust标准化】
目的：对异常值鲁棒
公式：(x - median) / IQR
适用：有异常值的数据

代码：
```python
from sklearn.preprocessing import RobustScaler

scaler = RobustScaler()
X_robust = scaler.fit_transform(X)
```

【验证要求】
1. 标准化前后统计量对比
2. 可视化数据分布（直方图/箱线图）
3. 检查是否异常值影响
4. 对比不同标准化方法的效果
```

---

## 🎯 场景3: 特征组合与交互

### 特征交叉完整模板

```
请创建特征交叉。

【交叉类型】

1. 数值特征相乘
   特征A × 特征B
   用途：捕捉交互效应
   
   代码：
   ```python
   df['A_x_B'] = df['A'] * df['B']
   ```

2. 数值特征相除
   特征A / 特征B
   用途：比率特征
   注意：除以0的处理
   
   代码：
   ```python
   df['A_div_B'] = df['A'] / (df['B'] + 1e-8)  # 加小值避免除0
   ```

3. 数值特征加减
   特征A - 特征B
   用途：差值特征

4. 指数和对数组合
   exp(A) × log(B)
   用途：非线性变换

【实现步骤】
1. 确定要交叉的特征对
2. 创建新特征
3. 验证新特征的有效性
4. 分析新特征与目标的关系

【验证要求】
1. 新特征统计描述
2. 新特征与目标的相关性
3. 新特征在模型中的重要性
4. 对比有/无新特征的模型性能
```

### 统计特征完整模板

```
请创建统计聚合特征。

【聚合类型】
基于类别变量的统计：

对于每个类别（如：用户ID、商品ID），计算：
1. 数值特征的统计量
   - count: 计数
   - mean: 均值
   - median: 中位数
   - std: 标准差
   - min: 最小值
   - max: 最大值
   - sum: 求和

2. 时间窗口统计
   - rolling_mean: 滚动均值
   - rolling_max: 滚动最大值
   - rolling_std: 滚动标准差

3. 滞后特征
   - shift(1): 前一个值
   - shift(7): 前7个值

代码示例：
```python
# 按用户ID分组统计
user_stats = df.groupby('user_id')['amount'].agg(['mean', 'count', 'std', 'max'])

# 重命名列
user_stats.columns = ['user_mean', 'user_count', 'user_std', 'user_max']

# 合并回原数据
df = df.merge(user_stats, left_on='user_id', right_index=True, how='left')
```

【验证要求】
1. 显示聚合后的统计量
2. 检查缺失值
3. 评估新特征的重要性
4. 可视化特征分布
```

---

## 📉 场景4: 降维算法

### PCA降维完整模板

```
请使用PCA进行降维。

【数据信息】
原始特征：【X】
特征数量：【原始维度】
目标维度：【K或保留方差比例】

【实现步骤】
1. 数据标准化（必须）
   ```python
   from sklearn.preprocessing import StandardScaler
   X_scaled = StandardScaler().fit_transform(X)
   ```

2. 确定主成分数量
   - 方法1：固定数量（如10个）
   - 方法2：解释方差比例（如95%）
   
   代码：
   ```python
   from sklearn.decomposition import PCA
   
   # 方法1：固定数量
   pca = PCA(n_components=10)
   
   # 方法2：保留95%方差
   pca = PCA(n_components=0.95)
   
   X_pca = pca.fit_transform(X_scaled)
   ```

3. 查看解释方差
   ```python
   print("解释方差比:", pca.explained_variance_ratio_)
   print("累积解释方差比:", pca.explained_variance_ratio_.cumsum())
   ```

4. 主成分载荷分析
   ```python
   loadings = pca.components_.T * np.sqrt(pca.explained_variance_)
   ```

【可视化要求】
1. 碎石图（确定最优成分数）
   - X轴：主成分数量
   - Y轴：累积解释方差比
   - 显示95%阈值线

2. 主成分载荷热力图
   - 显示原始特征与主成分的关系
   - 帮助解释主成分含义

3. 2D投影散点图
   - 用前两个主成分
   - 按类别着色

4. 3D投影散点图
   - 用前三个主成分
   - 可旋转查看

【验证要求】
1. 降维后的维度
2. 累积解释方差
3. 对比降维前后的模型性能
4. 降维带来的信息损失评估

【输出报告】
=== PCA降维报告 ===
原始特征数: 【数量】
降维后特征数: 【数量】
累积解释方差: 【值】%
信息保留率: 【值】%
性能对比:
  - 原始数据: 准确率【值】
  - PCA数据: 准确率【值】
  - 性能损失: 【值】%

【主成分解释】
PC1: 解释了【值】%的方差，主要由【特征A, 特征B】贡献
PC2: 解释了【值】%的方差，主要由【特征C, 特征D】贡献
...
```

### t-SNE降维完整模板

```
请使用t-SNE进行降维可视化。

【适用情况】
数据：高维数据（>10维）
目的：可视化聚类结构

【参数设置】
n_components: 2（2D）或3（3D）
perplexity: 30（5-50之间）
random_state: 42
n_iter: 1000

【实现步骤】
1. 数据标准化
2. 执行t-SNE
3. 可视化2D/3D散点图
4. 按类别着色

代码：
```python
from sklearn.manifold import TSNE

# 降维到2D
tsne = TSNE(n_components=2, perplexity=30, random_state=42)
X_tsne = tsne.fit_transform(X_scaled)

# 可视化
plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis')
plt.colorbar()
plt.show()
```

【验证要求】
1. 不同perplexity值的对比
2. 可视化聚类结构
3. 与PCA对比
```

### LDA降维完整模板

```
请使用LDA进行有监督降维。

【适用情况】
有标签数据（分类问题）
目的：最大化类间可分性

【实现步骤】
1. 标准化数据
2. 执行LDA
3. 降维可视化
4. 评估分类性能

代码：
```python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# LDA降维
lda = LinearDiscriminantAnalysis(n_components=2)
X_lda = lda.fit_transform(X_scaled, y)

# 可视化
plt.scatter(X_lda[:, 0], X_lda[:, 1], c=y, cmap='viridis')
plt.colorbar()
plt.show()
```

【验证要求】
1. 类间可分性
2. 降维后的分类性能
3. 与PCA对比
```

---

## 🎲 场景5: 特征重要性评估

### Permutation Importance完整模板

```
请计算Permutation Importance。

【方法说明】
打乱特征值，观察模型性能下降
性能下降越多，特征越重要

【实现步骤】
1. 训练基础模型
2. 计算基准性能
3. 对每个特征：
   - 打乱该特征的值
   - 重新评估模型
   - 计算性能下降
4. 排序特征重要性

代码：
```python
from sklearn.inspection import permutation_importance

# 训练模型
model.fit(X_train, y_train)

# 计算permutation importance
result = permutation_importance(model, X_test, y_test, 
                               n_repeats=30, random_state=42)

# 排序
sorted_idx = result.importances_mean.argsort()[::-1]
```

【可视化要求】
1. 特征重要性柱状图
2. 误差棒（重复采样）
3. 与模型内重要性对比
```

### SHAP值完整模板

```
请使用SHAP解释特征重要性。

【适用情况】
模型：任意机器学习模型
目的：解释单个预测的贡献

【实现步骤】
1. 安装shap库
2. 计算SHAP值
3. 可视化特征重要性
4. 解释单个预测

代码：
```python
import shap

# 创建explainer
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

# 可视化特征重要性
shap.summary_plot(shap_values, X_test)

# 解释单个预测
shap.force_plot(explainer.expected_value, shap_values[0,:], X_test[0,:])
```

【可视化要求】
1. Summary Plot（全局重要性）
2. Dependence Plot（特征依赖）
3. Force Plot（单样本解释）
4. Interaction Plot（特征交互）
```

---

## 💡 完整特征工程流程

### 端到端特征工程完整模板

```
请对数据集进行完整的特征工程流程。

【数据信息】
特征矩阵：【X】
目标变量：【y】
数据类型：【分类/回归】

【特征工程流程】

步骤1：数据清洗（5分钟）
- 删除缺失值过多的列（>50%）
- 删除单一值列（只有1个唯一值）
- 删除高相关列（相关系数>0.95）

步骤2：特征创建（10分钟）
- 多项式特征（degree=2）
- 对数变换（右偏特征）
- 交互特征（重要特征相乘）

步骤3：特征缩放（5分钟）
- 标准化数值特征
- 编码分类特征

步骤4：特征选择（15分钟）
- 移除低方差特征
- 使用模型选择Top K
- RFE递归消除

步骤5：降维（可选，10分钟）
- PCA降维
- t-SNE可视化

步骤6：验证（10分钟）
- 对比原始vs工程化后性能
- 学习曲线
- 验证曲线

【代码要求】
创建Pipeline
使用sklearn的Pipeline和FeatureUnion
网格搜索最优参数
交叉验证
详细注释

【输出报告】
=== 完整特征工程报告 ===
原始特征数: 【数量】
工程化后特征数: 【数量】
删除的特征: 【列表】
创建的特征: 【列表】
模型性能:
  - 原始: 准确率【值】
  - 工程化: 准确率【值】
  - 提升: 【值】%

【最终特征列表】
1. 【特征名】: 重要性【值】
2. 【特征名】: 重要性【值】
...
```

---

## ⚡ 快速特征工程模板

### 快速特征工程完整模板

```
请快速进行特征工程。

【数据】X, y

【流程】
1. 删除方差<0.01的特征
2. 标准化
3. PCA降维到95%方差
4. 选择随机森林Top 10特征
5. 训练模型并评估

【要求】
- 显示每步的特征数
- 可视化特征重要性
- 输出最终特征列表
- 性能对比
```

---

## 📋 特征工程检查清单

使用本清单确保不遗漏重要步骤：

### 数据理解 ☐
- [ ] 特征数量和类型
- [ ] 特征统计分布
- [ ] 缺失值模式
- [ ] 特征相关性

### 特征创建 ☐
- [ ] 多项式特征
- [ ] 交互特征
- [ ] 统计特征
- [ ] 时间特征

### 特征选择 ☐
- [ ] 删除低方差
- [ ] 删除高相关
- [ ] 单变量选择
- [ ] 模型选择
- [ ] RFE选择

### 特征变换 ☐
- [ ] 标准化
- [ ] 归一化
- [ ] 对数变换
- [ ] Box-Cox

### 降维 ☐
- [ ] PCA
- [ ] t-SNE
- [ ] LDA

### 验证 ☐
- [ ] 交叉验证
- [ ] 性能对比
- [ ] 特征稳定性
- [ ] 业务合理性

---

**文件结束**

本文件涵盖了特征工程的完整流程，从数据理解到特征选择、变换、降维，每个模板都包含详细的实现步骤和验证要求。
