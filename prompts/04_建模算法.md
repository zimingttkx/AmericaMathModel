# å»ºæ¨¡ç®—æ³• AI Prompt æ¨¡æ¿ï¼ˆè¯¦ç»†ç‰ˆï¼‰

> **ä½¿ç”¨è¯´æ˜**: æœ¬æ–‡ä»¶åŒ…å«ç®—æ³•é€‰æ‹©ã€æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€ä¼˜åŒ–ç®—æ³•ç­‰çš„è¯¦ç»†Promptæ¨¡æ¿ã€‚

---

## ğŸ¯ åœºæ™¯1: ç®—æ³•é€‰æ‹©å»ºè®®

### å®Œæ•´ç®—æ³•é€‰æ‹©Prompt

```
æˆ‘æœ‰ä¸€ä¸ªæ•°å­¦å»ºæ¨¡é—®é¢˜ï¼Œéœ€è¦ä½ æ¨èæœ€åˆé€‚çš„ç®—æ³•ã€‚

ã€é—®é¢˜æè¿°ã€‘
é—®é¢˜ç±»å‹ï¼šè¯·ä»ä»¥ä¸‹ç±»å‹ä¸­é€‰æ‹©
  - åˆ†ç±»é—®é¢˜ï¼ˆäºŒåˆ†ç±»/å¤šåˆ†ç±»ï¼‰
  - å›å½’é—®é¢˜ï¼ˆæ•°å€¼é¢„æµ‹ï¼‰
  - èšç±»é—®é¢˜ï¼ˆæ— ç›‘ç£åˆ†ç»„ï¼‰
  - é™ç»´é—®é¢˜ï¼ˆå‡å°‘ç‰¹å¾ï¼‰
  - æ—¶é—´åºåˆ—é¢„æµ‹
  - ä¼˜åŒ–é—®é¢˜ï¼ˆæœ€ä¼˜åŒ–ç›®æ ‡ï¼‰
  - å›¾è®ºé—®é¢˜ï¼ˆç½‘ç»œã€è·¯å¾„ï¼‰
  - è¯„ä»·é—®é¢˜ï¼ˆå¤šæŒ‡æ ‡å†³ç­–ï¼‰

ã€æ•°æ®ä¿¡æ¯ã€‘
æ•°æ®è§„æ¨¡ï¼š
  - æ ·æœ¬æ•°ï¼šã€è¡Œæ•°ã€‘
  - ç‰¹å¾æ•°ï¼šã€åˆ—æ•°ã€‘
  - ç›®æ ‡å˜é‡ï¼šã€ç±»åˆ«æ•°/æ•°å€¼ã€‘

æ•°æ®ç±»å‹ï¼š
  - ç‰¹å¾ç±»å‹ï¼šã€æ•°å€¼/åˆ†ç±»/æ··åˆã€‘
  - æ ‡ç­¾ç±»å‹ï¼šã€äºŒåˆ†ç±»/å¤šåˆ†ç±»/è¿ç»­å€¼ã€‘

æ•°æ®è´¨é‡ï¼š
  - ç¼ºå¤±å€¼æ¯”ä¾‹ï¼šã€å¤§çº¦æ¯”ä¾‹ã€‘
  - å¼‚å¸¸å€¼æƒ…å†µï¼šã€æœ‰/æ— /ä¸¥é‡ã€‘
  - å™ªå£°æ°´å¹³ï¼šã€ä½/ä¸­/é«˜ã€‘
  - æ•°æ®å¹³è¡¡ï¼šã€å¹³è¡¡/ä¸å¹³è¡¡ã€‘

ã€çº¦æŸæ¡ä»¶ã€‘
è®¡ç®—èµ„æºï¼š
  - è®­ç»ƒæ—¶é—´é™åˆ¶ï¼šã€å‡ åˆ†é’Ÿ/å‡ å°æ—¶ã€‘
  - å†…å­˜é™åˆ¶ï¼šã€å……è¶³/æœ‰é™ã€‘
  - æ˜¯å¦éœ€è¦å®æ—¶é¢„æµ‹ï¼šã€æ˜¯/å¦ã€‘

æ¨¡å‹è¦æ±‚ï¼š
  - å¯è§£é‡Šæ€§ï¼šã€å¿…é¡»/é‡è¦/ä¸é‡è¦ã€‘
  - å‡†ç¡®åº¦ä¼˜å…ˆçº§ï¼šã€æœ€é«˜/ä¸­ç­‰/ä¸€èˆ¬ã€‘
  - é²æ£’æ€§è¦æ±‚ï¼šã€é«˜/ä¸­/ä½ã€‘
  - é›†æˆéš¾åº¦ï¼šã€ç®€å•/ä¸­ç­‰/å¤æ‚ã€‘

ã€ç‰¹æ®Šè¦æ±‚ã€‘
- æ˜¯å¦éœ€è¦å¤„ç†éçº¿æ€§å…³ç³»ï¼šã€æ˜¯/å¦ã€‘
- æ˜¯å¦éœ€è¦å¤„ç†æ—¶é—´ä¾èµ–ï¼šã€æ˜¯/å¦ã€‘
- æ˜¯å¦éœ€è¦å¤„ç†ç©ºé—´æ•°æ®ï¼šã€æ˜¯/å¦ã€‘
- æ˜¯å¦éœ€è¦å¤„ç†æ–‡æœ¬æ•°æ®ï¼šã€æ˜¯/å¦ã€‘
- æ˜¯å¦éœ€è¦å¤„ç†å›¾åƒæ•°æ®ï¼šã€æ˜¯/å¦ã€‘
- æ˜¯å¦éœ€è¦å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼šã€æ˜¯/å¦ã€‘

ã€è¯·æä¾›ã€‘
1. æ¨è3-5ä¸ªæœ€é€‚åˆçš„ç®—æ³•
2. æ¯ä¸ªç®—æ³•çš„è¯¦ç»†è¯´æ˜ï¼š
   - ç®—æ³•åŸç†ç®€ä»‹ï¼ˆ1-2å¥è¯ï¼‰
   - ä¼˜ç‚¹ï¼ˆ3-5ç‚¹ï¼‰
   - ç¼ºç‚¹ï¼ˆ2-3ç‚¹ï¼‰
   - é€‚ç”¨åŸå› ï¼ˆä¸ºä»€ä¹ˆé€‚åˆè¿™ä¸ªé—®é¢˜ï¼‰
3. ç®—æ³•é€‰æ‹©ç†ç”±ï¼ˆæ’åºä¾æ®ï¼‰
4. å®ç°éš¾åº¦è¯„ä¼°ï¼ˆ1-5æ˜Ÿï¼‰
5. é¢„æœŸæ€§èƒ½ï¼ˆå¦‚æœæœ‰ç±»ä¼¼æ¡ˆä¾‹ï¼‰

ã€è¾“å‡ºæ ¼å¼ã€‘
=== ç®—æ³•æ¨èæŠ¥å‘Š ===

é—®é¢˜ç±»å‹ï¼šã€ç±»å‹ã€‘

æ¨èç®—æ³•ï¼š

1. ã€ç®—æ³•1åç§°ã€‘
   - åŸç†ï¼šã€ç®€è¿°ã€‘
   - ä¼˜ç‚¹ï¼š
     * ã€ä¼˜ç‚¹1ã€‘
     * ã€ä¼˜ç‚¹2ã€‘
     * ã€ä¼˜ç‚¹3ã€‘
   - ç¼ºç‚¹ï¼š
     * ã€ç¼ºç‚¹1ã€‘
     * ã€ç¼ºç‚¹2ã€‘
   - é€‚ç”¨åŸå› ï¼šã€åŸå› ã€‘
   - å®ç°éš¾åº¦ï¼šã€æ˜Ÿçº§ã€‘
   - é¢„æœŸæ€§èƒ½ï¼šã€ä¼°è®¡ã€‘

2. ã€ç®—æ³•2åç§°ã€‘
   ...

ã€æ¨èé¡ºåºã€‘
é¦–é€‰ï¼šã€ç®—æ³•1ã€‘
å¤‡é€‰ï¼šã€ç®—æ³•2, ç®—æ³•3ã€‘

ã€ç†ç”±ã€‘
ã€è¯¦ç»†è¯´æ˜æ¨èç†ç”±ã€‘
```

### å®é™…ä½¿ç”¨ç¤ºä¾‹ï¼ˆåˆ†ç±»é—®é¢˜ï¼‰

```
ã€é—®é¢˜æè¿°ã€‘
é—®é¢˜ç±»å‹ï¼šäºŒåˆ†ç±»
æ•°æ®è§„æ¨¡ï¼š10000è¡Œ Ã— 20åˆ—
ç›®æ ‡ï¼šé¢„æµ‹å®¢æˆ·æ˜¯å¦æµå¤±ï¼ˆæ˜¯/å¦ï¼‰

ã€æ•°æ®ä¿¡æ¯ã€‘
ç‰¹å¾ç±»å‹ï¼šæ··åˆï¼ˆæ•°å€¼+åˆ†ç±»ï¼‰
- æ•°å€¼ï¼šå¹´é¾„ã€æ”¶å…¥ã€æ¶ˆè´¹é‡‘é¢ç­‰
- åˆ†ç±»ï¼šæ€§åˆ«ã€åœ°åŒºã€ä¼šå‘˜ç­‰çº§ç­‰

æ•°æ®è´¨é‡ï¼š
- ç¼ºå¤±å€¼ï¼š5%
- æ•°æ®ä¸å¹³è¡¡ï¼šæµå¤±ç‡15%
- å™ªå£°ï¼šä¸­ç­‰

ã€çº¦æŸæ¡ä»¶ã€‘
- è®­ç»ƒæ—¶é—´ï¼š1å°æ—¶å†…
- å¯è§£é‡Šæ€§ï¼šé«˜ï¼ˆéœ€è¦å‘ä¸šåŠ¡éƒ¨é—¨è§£é‡Šï¼‰
- å‡†ç¡®åº¦ï¼šä¼˜å…ˆçº§æœ€é«˜

ã€è¯·æ¨èã€‘
æœ€é€‚åˆçš„3-5ä¸ªç®—æ³•ï¼Œå¹¶è¯´æ˜åŸå› ã€‚
```

### å®é™…ä½¿ç”¨ç¤ºä¾‹ï¼ˆå›å½’é—®é¢˜ï¼‰

```
ã€é—®é¢˜æè¿°ã€‘
é—®é¢˜ç±»å‹ï¼šå›å½’é¢„æµ‹
æ•°æ®è§„æ¨¡ï¼š5000è¡Œ Ã— 15åˆ—
ç›®æ ‡ï¼šé¢„æµ‹æˆ¿ä»·ï¼ˆä¸‡å…ƒï¼‰

ã€æ•°æ®ä¿¡æ¯ã€‘
ç‰¹å¾ï¼šé¢ç§¯ã€æˆ¿é—´æ•°ã€ä½ç½®ã€æˆ¿é¾„ç­‰
è´¨é‡ï¼šå¥½ï¼Œç¼ºå¤±å€¼å°‘

ã€çº¦æŸã€‘
- éœ€è¦ç‰¹å¾é‡è¦æ€§
- è®­ç»ƒæ—¶é—´æœ‰é™
- é¢„æµ‹è¯¯å·®è¦æ±‚ä½

ã€è¯·æ¨èã€‘æœ€é€‚åˆçš„ç®—æ³•ã€‚
```

---

## ğŸ¤– åœºæ™¯2: æœºå™¨å­¦ä¹ ç®—æ³•å®ç°

### é€»è¾‘å›å½’å®Œæ•´å®ç°Prompt

```
è¯·å®ç°ä¸€ä¸ªå®Œæ•´çš„é€»è¾‘å›å½’åˆ†ç±»æ¨¡å‹ã€‚

ã€æ•°æ®ä¿¡æ¯ã€‘
è®­ç»ƒé›†ï¼šX_train, y_train
æµ‹è¯•é›†ï¼šX_test, y_test
ç‰¹å¾æ•°ï¼šã€Kã€‘
ç±»åˆ«æ•°ï¼šã€2ã€‘ï¼ˆäºŒåˆ†ç±»ï¼‰æˆ–ã€>2ã€‘ï¼ˆå¤šåˆ†ç±»ï¼‰

ã€æ¨¡å‹å‚æ•°ã€‘
penalty: l1, l2, elasticnet, none
C: æ­£åˆ™åŒ–å¼ºåº¦çš„å€’æ•°ã€1.0ã€‘
solver: liblinearï¼ˆå°æ•°æ®ï¼‰æˆ– lbfgsï¼ˆå¤§æ•°æ®ï¼‰
max_iter: 100
random_state: 42
class_weight: balancedï¼ˆå¦‚æœæœ‰ç±»åˆ«ä¸å¹³è¡¡ï¼‰

ã€å®ç°æ­¥éª¤ã€‘

æ­¥éª¤1ï¼šæ•°æ®é¢„å¤„ç†
```python
# æ ‡å‡†åŒ–æ•°å€¼ç‰¹å¾
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ç¼–ç åˆ†ç±»å˜é‡ï¼ˆå¦‚æœæœ‰ï¼‰
from sklearn.preprocessing import OneHotEncoder
```

æ­¥éª¤2ï¼šè®­ç»ƒæ¨¡å‹
```python
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(
    penalty='l2',
    C=1.0,
    solver='lbfgs',
    max_iter=100,
    random_state=42
)

model.fit(X_train_scaled, y_train)
```

æ­¥éª¤3ï¼šé¢„æµ‹
```python
y_pred = model.predict(X_test_scaled)
y_pred_proba = model.predict_proba(X_test_scaled)
```

æ­¥éª¤4ï¼šè¯„ä¼°
```python
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve

# è®¡ç®—æ‰€æœ‰æŒ‡æ ‡
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred_proba[:, 1])
```

æ­¥éª¤5ï¼šå¯è§†åŒ–
```python
# æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾
# ROCæ›²çº¿
# ç‰¹å¾ç³»æ•°æ¡å½¢å›¾
```

ã€è¾“å‡ºè¦æ±‚ã€‘
1. å®Œæ•´çš„å¯è¿è¡Œä»£ç 
2. è¯¦ç»†çš„ä¸­æ–‡æ³¨é‡Š
3. æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡
4. å¯è§†åŒ–å›¾è¡¨
5. æ¨¡å‹å‚æ•°è§£é‡Šï¼ˆç³»æ•°ï¼‰

ã€æ¨¡å‹è§£é‡Šã€‘
- ç‰¹å¾ç³»æ•°åˆ†æ
- ORå€¼è®¡ç®—ï¼ˆå¯¹äºäºŒåˆ†ç±»ï¼‰
- ç‰¹å¾é‡è¦æ€§æ’åº
```

### éšæœºæ£®æ—å®Œæ•´å®ç°Prompt

```
è¯·å®ç°éšæœºæ£®æ—åˆ†ç±»å™¨/å›å½’å™¨ã€‚

ã€æ•°æ®ä¿¡æ¯ã€‘
é—®é¢˜ç±»å‹ï¼šåˆ†ç±»æˆ–å›å½’
X_train, y_train, X_test, y_test

ã€æ¨¡å‹å‚æ•°ã€‘
n_estimators: 100ï¼ˆæ ‘çš„æ•°é‡ï¼‰
max_depth: Noneæˆ–æŒ‡å®šï¼ˆé™åˆ¶æ ‘æ·±åº¦ï¼‰
min_samples_split: 2ï¼ˆåˆ†è£‚æœ€å°æ ·æœ¬æ•°ï¼‰
min_samples_leaf: 1ï¼ˆå¶èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°ï¼‰
max_features: 'sqrt'æˆ–'log2'æˆ–None
random_state: 42
n_jobs: -1ï¼ˆä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒï¼‰

ã€å®ç°æ­¥éª¤ã€‘

æ­¥éª¤1ï¼šè®­ç»ƒæ¨¡å‹
```python
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(
    n_estimators=100,
    max_depth=None,
    min_samples_split=2,
    min_samples_leaf=1,
    max_features='sqrt',
    random_state=42,
    n_jobs=-1
)

model.fit(X_train, y_train)
```

æ­¥éª¤2ï¼šç‰¹å¾é‡è¦æ€§åˆ†æ
```python
importances = model.feature_importances_
indices = np.argsort(importances)[::-1]

# å¯è§†åŒ–
plt.figure(figsize=(10, 6))
plt.title('Feature Importances')
plt.bar(range(len(importances)), importances[indices])
plt.xticks(range(len(importances)), [feature_names[i] for i in indices])
plt.tight_layout()
plt.show()
```

æ­¥éª¤3ï¼šè¯„ä¼°æ€§èƒ½
```python
from sklearn.metrics import classification_report, confusion_matrix

y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
```

æ­¥éª¤4ï¼šè¶…å‚æ•°è°ƒä¼˜
```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

print("æœ€ä½³å‚æ•°:", grid_search.best_params_)
print("æœ€ä½³å¾—åˆ†:", grid_search.best_score_)
```

ã€è¾“å‡ºè¦æ±‚ã€‘
1. å®Œæ•´ä»£ç 
2. ç‰¹å¾é‡è¦æ€§æ’åº
3. ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–
4. æ€§èƒ½è¯„ä¼°æŠ¥å‘Š
5. è¶…å‚æ•°è°ƒä¼˜ç»“æœ
```

### XGBoostå®Œæ•´å®ç°Prompt

```
è¯·å®ç°XGBoostæ¨¡å‹ã€‚

ã€æ•°æ®ä¿¡æ¯ã€‘
é—®é¢˜ç±»å‹ï¼šåˆ†ç±»æˆ–å›å½’
æ•°æ®ï¼šX_train, y_train, X_test, y_test

ã€æ¨¡å‹å‚æ•°ã€‘
learning_rate: 0.1ï¼ˆå­¦ä¹ ç‡ï¼‰
max_depth: 6ï¼ˆæ ‘çš„æœ€å¤§æ·±åº¦ï¼‰
n_estimators: 100ï¼ˆæ ‘çš„æ•°é‡ï¼‰
min_child_weight: 1ï¼ˆæœ€å°å­èŠ‚ç‚¹æƒé‡ï¼‰
subsample: 0.8ï¼ˆæ ·æœ¬é‡‡æ ·æ¯”ä¾‹ï¼‰
colsample_bytree: 0.8ï¼ˆç‰¹å¾é‡‡æ ·æ¯”ä¾‹ï¼‰
gamma: 0ï¼ˆæœ€å°æŸå¤±å‡å°‘ï¼‰
reg_alpha: 0ï¼ˆL1æ­£åˆ™åŒ–ï¼‰
reg_lambda: 1ï¼ˆL2æ­£åˆ™åŒ–ï¼‰
random_state: 42

ã€å®ç°æ­¥éª¤ã€‘

æ­¥éª¤1ï¼šè®­ç»ƒæ¨¡å‹
```python
import xgboost as xgb

model = xgb.XGBClassifier(
    learning_rate=0.1,
    max_depth=6,
    n_estimators=100,
    min_child_weight=1,
    subsample=0.8,
    colsample_bytree=0.8,
    gamma=0,
    reg_alpha=0,
    reg_lambda=1,
    random_state=42,
    use_label_encoder=False
)

model.fit(X_train, y_train)
```

æ­¥éª¤2ï¼šæ—©åœå’ŒéªŒè¯é›†
```python
eval_set = [(X_train, y_train), (X_test, y_test)]
model.fit(X_train, y_train,
          eval_set=eval_set,
          early_stopping_rounds=10,
          verbose=False)
```

æ­¥éª¤3ï¼šç‰¹å¾é‡è¦æ€§
```python
from xgboost import plot_importance

plot_importance(model)
plt.show()

# æˆ–ä½¿ç”¨plot_importance(model, importance_type='gain')
```

æ­¥éª¤4ï¼šå­¦ä¹ æ›²çº¿
```python
evals_result = model.evals_result()
training_rounds = len(evals_result['validation_0']['logloss'])

plt.figure(figsize=(10, 6))
plt.plot(range(training_rounds), evals_result['validation_0']['logloss'], label='Train')
plt.plot(range(training_rounds), evals_result['validation_1']['logloss'], label='Test')
plt.xlabel('Rounds')
plt.ylabel('Log Loss')
plt.legend()
plt.show()
```

ã€è¾“å‡ºè¦æ±‚ã€‘
1. å®Œæ•´ä»£ç 
2. ç‰¹å¾é‡è¦æ€§åˆ†æ
3. å­¦ä¹ æ›²çº¿å¯è§†åŒ–
4. æ€§èƒ½è¯„ä¼°
5. SHAPå€¼è§£é‡Šï¼ˆå¯é€‰ï¼‰
```

### SVMå®Œæ•´å®ç°Prompt

```
è¯·å®ç°æ”¯æŒå‘é‡æœºã€‚

ã€æ•°æ®ä¿¡æ¯ã€‘
é—®é¢˜ç±»å‹ï¼šåˆ†ç±»æˆ–å›å½’
æ•°æ®ï¼šX_train, y_train, X_test, y_test

ã€æ¨¡å‹å‚æ•°ã€‘
kernel: 'linear'ï¼ˆçº¿æ€§ï¼‰/ 'rbf'ï¼ˆé«˜æ–¯ï¼‰/ 'poly'ï¼ˆå¤šé¡¹å¼ï¼‰
C: 1.0ï¼ˆæ­£åˆ™åŒ–å‚æ•°ï¼‰
gamma: 'scale'æˆ–'auto'ï¼ˆRBFæ ¸å‚æ•°ï¼‰
degree: 3ï¼ˆå¤šé¡¹å¼æ¬¡æ•°ï¼‰
probability: Trueï¼ˆéœ€è¦æ¦‚ç‡è¾“å‡ºï¼‰

ã€å®ç°æ­¥éª¤ã€‘

æ­¥éª¤1ï¼šæ•°æ®æ ‡å‡†åŒ–ï¼ˆå¿…é¡»ï¼‰
```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

æ­¥éª¤2ï¼šè®­ç»ƒSVM
```python
from sklearn.svm import SVC

model = SVC(kernel='rbf', C=1.0, gamma='scale', 
           probability=True, random_state=42)
model.fit(X_train_scaled, y_train)
```

æ­¥éª¤3ï¼šè¯„ä¼°
```python
y_pred = model.predict(X_test_scaled)
y_proba = model.predict_proba(X_test_scaled)

# æ”¯æŒå‘é‡
print("æ”¯æŒå‘é‡æ•°é‡:", model.n_support_)
print("æ”¯æŒå‘é‡çš„ç´¢å¼•:", model.support_)
```

ã€è¾“å‡ºè¦æ±‚ã€‘
1. å®Œæ•´ä»£ç 
2. æ”¯æŒå‘é‡åˆ†æ
3. å†³ç­–è¾¹ç•Œå¯è§†åŒ–ï¼ˆ2Dæ•°æ®ï¼‰
4. æ€§èƒ½è¯„ä¼°
```

---

## ğŸ§  åœºæ™¯3: æ·±åº¦å­¦ä¹ ç®—æ³•

### ç¥ç»ç½‘ç»œå®Œæ•´å®ç°Prompt

```
è¯·ç”¨Keras/TensorFlowæ„å»ºç¥ç»ç½‘ç»œã€‚

ã€ç½‘ç»œæ¶æ„ã€‘
è¾“å…¥å±‚ï¼šã€ç‰¹å¾æ•°ã€‘ä¸ªç¥ç»å…ƒ
éšè—å±‚1ï¼šã€64ã€‘ä¸ªç¥ç»å…ƒï¼Œæ¿€æ´»å‡½æ•°ReLU
Dropoutï¼šã€0.3ã€‘
éšè—å±‚2ï¼šã€32ã€‘ä¸ªç¥ç»å…ƒï¼Œæ¿€æ´»å‡½æ•°ReLU
Dropoutï¼šã€0.3ã€‘
è¾“å‡ºå±‚ï¼šã€ç±»åˆ«æ•°ã€‘ä¸ªç¥ç»å…ƒï¼Œæ¿€æ´»å‡½æ•°softmax

ã€ç¼–è¯‘ã€‘
ä¼˜åŒ–å™¨ï¼šAdam
æŸå¤±å‡½æ•°ï¼šcategorical_crossentropyï¼ˆåˆ†ç±»ï¼‰/ mseï¼ˆå›å½’ï¼‰
è¯„ä¼°æŒ‡æ ‡ï¼šaccuracy

ã€è®­ç»ƒã€‘
epochsï¼šã€50ã€‘
batch_sizeï¼šã€32ã€‘
validation_splitï¼šã€0.2ã€‘
early_stoppingï¼špatience=5

ã€å®ç°æ­¥éª¤ã€‘

æ­¥éª¤1ï¼šæ„å»ºæ¨¡å‹
```python
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(input_dim,)),
    layers.Dropout(0.3),
    layers.Dense(32, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(num_classes, activation='softmax')
])
```

æ­¥éª¤2ï¼šç¼–è¯‘æ¨¡å‹
```python
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
```

æ­¥éª¤3ï¼šè®­ç»ƒ
```python
history = model.fit(X_train, y_train,
                    epochs=50,
                    batch_size=32,
                    validation_split=0.2,
                    verbose=1)
```

æ­¥éª¤4ï¼šè¯„ä¼°å’Œå¯è§†åŒ–
```python
# å­¦ä¹ æ›²çº¿
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])

# è¯„ä¼°
test_loss, test_acc = model.evaluate(X_test, y_test)
```

ã€è¾“å‡ºè¦æ±‚ã€‘
1. å®Œæ•´ä»£ç 
2. æ¨¡å‹ç»“æ„å›¾
3. è®­ç»ƒæ›²çº¿å¯è§†åŒ–
4. æ€§èƒ½è¯„ä¼°
```

### LSTMæ—¶é—´åºåˆ—å®Œæ•´å®ç°Prompt

```
è¯·ç”¨LSTMé¢„æµ‹æ—¶é—´åºåˆ—ã€‚

ã€æ•°æ®å‡†å¤‡ã€‘
æ—¶é—´åºåˆ—ï¼šã€æ—¥æœŸåˆ—, å€¼åˆ—ã€‘
æ—¶é—´æ­¥é•¿ï¼šã€10ã€‘ï¼ˆä½¿ç”¨è¿‡å»10ä¸ªç‚¹é¢„æµ‹ä¸‹ä¸€ä¸ªï¼‰
é¢„æµ‹æ­¥é•¿ï¼šã€1ã€‘ï¼ˆé¢„æµ‹æœªæ¥1ä¸ªç‚¹ï¼‰

ã€å®ç°æ­¥éª¤ã€‘

æ­¥éª¤1ï¼šæ„é€ æ—¶é—´åºåˆ—æ ·æœ¬
```python
def create_sequences(data, seq_length):
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:i+seq_length])
        y.append(data[i+seq_length])
    return np.array(X), np.array(y)

X, y = create_sequences(values, seq_length=10)
X = X.reshape(X.shape[0], X.shape[1], 1)
```

æ­¥éª¤2ï¼šæ„å»ºLSTMæ¨¡å‹
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

model = Sequential([
    LSTM(50, activation='relu', input_shape=(seq_length, 1)),
    Dense(1)
])
```

æ­¥éª¤3ï¼šè®­ç»ƒå’Œé¢„æµ‹
```python
model.compile(optimizer='adam', loss='mse')
history = model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)
```

æ­¥éª¤4ï¼šé¢„æµ‹
```python
predictions = model.predict(X_test)
```

ã€è¾“å‡ºè¦æ±‚ã€‘
1. å®Œæ•´ä»£ç 
2. é¢„æµ‹å¯è§†åŒ–
3. è¯„ä¼°æŒ‡æ ‡ï¼ˆRMSE, MAE, RÂ²ï¼‰
```

---

## ğŸ“ˆ åœºæ™¯4: æ—¶é—´åºåˆ—ç®—æ³•

### ARIMAå®Œæ•´å®ç°Prompt

```
è¯·å®ç°ARIMAæ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹ã€‚

ã€æ•°æ®ä¿¡æ¯ã€‘
æ—¶é—´åºåˆ—ï¼šã€æ—¥æœŸåˆ—, å€¼åˆ—ã€‘
æ•°æ®é¢‘ç‡ï¼šæ—¥('D')ã€å‘¨('W')ã€æœˆ('M')ã€å­£åº¦('Q')
é¢„æµ‹æœŸæ•°ï¼šã€æœªæ¥NæœŸã€‘

ã€å®ç°æ­¥éª¤ã€‘

æ­¥éª¤1ï¼šæ£€æŸ¥å¹³ç¨³æ€§
```python
from statsmodels.tsa.stattools import adfuller

result = adfuller(data['value'])
print('ADF Statistic:', result[0])
print('p-value:', result[1])

# å¦‚æœp>0.05ï¼Œéœ€è¦å·®åˆ†
```

æ­¥éª¤2ï¼šç¡®å®šARIMAå‚æ•°
```python
from statsmodels.tsa.stattools import acf, pacf
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# ç»˜åˆ¶ACFå’ŒPACFå›¾
plot_acf(data['value'])
plot_pacf(data['value'])

# æ ¹æ®ACFç¡®å®šMA(q)ï¼ŒPACFç¡®å®šAR(p)
```

æ­¥éª¤3ï¼šè®­ç»ƒARIMA
```python
from statsmodels.tsa.arima.model import ARIMA

model = ARIMA(data['value'], order=(p, d, q))
model_fit = model.fit()
```

æ­¥éª¤4ï¼šé¢„æµ‹
```python
forecast = model_fit.forecast(steps=N)
forecast_confint = model_fit.get_forecast(steps=N, alpha=0.05)
```

æ­¥éª¤5ï¼šå¯è§†åŒ–
```python
plt.figure(figsize=(12, 6))
plt.plot(data['value'], label='Historical')
plt.plot(forecast.index, forecast, label='Forecast')
plt.fill_between(forecast_confint.index,
                 forecast_confint[:, 0],
                 forecast_confint[:, 1],
                 alpha=0.2, label='95% CI')
plt.legend()
plt.show()
```

ã€è¾“å‡ºè¦æ±‚ã€‘
1. å®Œæ•´ä»£ç 
2. å¹³ç¨³æ€§æ£€éªŒç»“æœ
3. ACFå’ŒPACFå›¾
4. é¢„æµ‹ç»“æœå¯è§†åŒ–
5. æ¨¡å‹æ‘˜è¦ï¼ˆAIC, BICï¼‰
```

---

## ğŸ² åœºæ™¯5: èšç±»ç®—æ³•

### K-Meanså®Œæ•´å®ç°Prompt

```
è¯·å®ç°K-Meansèšç±»ç®—æ³•ã€‚

ã€æ•°æ®ä¿¡æ¯ã€‘
ç‰¹å¾çŸ©é˜µï¼šã€Xã€‘
èšç±»æ•°é‡ï¼šã€3ã€‘æˆ–ä½¿ç”¨è‚˜éƒ¨æ³•åˆ™ç¡®å®š

ã€å®ç°æ­¥éª¤ã€‘

æ­¥éª¤1ï¼šç¡®å®šæœ€ä½³Kå€¼ï¼ˆè‚˜éƒ¨æ³•åˆ™ï¼‰
```python
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

inertias = []
silhouettes = []
K_range = range(2, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X)
    inertias.append(kmeans.inertia_)
    silhouettes.append(silhouette_score(X, kmeans.labels_))

# ç»˜åˆ¶è‚˜éƒ¨æ³•åˆ™å›¾
plt.plot(K_range, inertias, 'bo-')
plt.xlabel('k')
plt.ylabel('Inertia')
plt.show()

# ç»˜åˆ¶è½®å»“ç³»æ•°å›¾
plt.plot(K_range, silhouettes, 'bo-')
plt.xlabel('k')
plt.ylabel('Silhouette Score')
plt.show()
```

æ­¥éª¤2ï¼šè®­ç»ƒK-Means
```python
kmeans = KMeans(n_clusters=3, init='k-means++', 
                n_init=10, max_iter=300, random_state=42)
kmeans.fit(X)
```

æ­¥éª¤3ï¼šèšç±»ä¸­å¿ƒ
```python
centers = kmeans.cluster_centers_
print("èšç±»ä¸­å¿ƒ:")
print(centers)
```

æ­¥éª¤4ï¼šå¯è§†åŒ–ï¼ˆ2Dï¼‰
```python
plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_)
plt.scatter(centers[:, 0], centers[:, 1], c='red', marker='x', s=200)
plt.show()
```

æ­¥éª¤5ï¼šè¯„ä¼°
```python
from sklearn.metrics import silhouette_score, calinski_harabasz_score

silhouette = silhouette_score(X, kmeans.labels_)
calinski = calinski_harabasz_score(X, kmeans.labels_)

print(f"Silhouette Score: {silhouette}")
print(f"Calinski-Harabasz Score: {calinski}")
```

ã€è¾“å‡ºè¦æ±‚ã€‘
1. å®Œæ•´ä»£ç 
2. æœ€ä½³Kå€¼åˆ†æ
3. èšç±»ä¸­å¿ƒ
4. 2D/3Då¯è§†åŒ–
5. èšç±»è¯„ä¼°æŒ‡æ ‡
```

### å±‚æ¬¡èšç±»å®Œæ•´å®ç°Prompt

```
è¯·å®ç°å±‚æ¬¡èšç±»ã€‚

ã€æ–¹æ³•ã€‘
linkage: 'ward'ï¼ˆæœ€å°æ–¹å·®æ³•ï¼‰/ 'complete'ï¼ˆæœ€é•¿è·ç¦»ï¼‰/ 'average'ï¼ˆå¹³å‡è·ç¦»ï¼‰
metric: 'euclidean'ï¼ˆæ¬§æ°è·ç¦»ï¼‰/ 'manhattan'ï¼ˆæ›¼å“ˆé¡¿è·ç¦»ï¼‰

ã€å®ç°æ­¥éª¤ã€‘

æ­¥éª¤1ï¼šè®¡ç®—è·ç¦»çŸ©é˜µ
```python
from scipy.cluster.hierarchy import linkage, dendrogram
from scipy.spatial.distance import pdist

# è®¡ç®—è·ç¦»çŸ©é˜µ
distance_matrix = pdist(X, metric='euclidean')
```

æ­¥éª¤2ï¼šå±‚æ¬¡èšç±»
```python
# æ‰§è¡Œå±‚æ¬¡èšç±»
Z = linkage(distance_matrix, method='ward')
```

æ­¥éª¤3ï¼šç»˜åˆ¶æ ‘çŠ¶å›¾
```python
plt.figure(figsize=(12, 8))
dendrogram(Z, truncate_mode='lastp', p=20)
plt.show()
```

æ­¥éª¤4ï¼šç¡®å®šç±»åˆ«æ•°
```python
from scipy.cluster.hierarchy import fcluster

# è®¾å®šè·ç¦»é˜ˆå€¼æˆ–ç±»åˆ«æ•°
max_d = 50  # æˆ–æŒ‡å®šèšç±»æ•°k=3
clusters = fcluster(Z, criterion='distance', t=max_d)
```

æ­¥éª¤5ï¼šè¯„ä¼°
```python
from sklearn.metrics import silhouette_score
score = silhouette_score(X, clusters)
print(f"Silhouette Score: {score}")
```

ã€è¾“å‡ºè¦æ±‚ã€‘
1. å®Œæ•´ä»£ç 
2. æ ‘çŠ¶å›¾
3. èšç±»åˆ†é…ç»“æœ
4. è¯„ä¼°æŒ‡æ ‡
```

---

## ğŸ”„ åœºæ™¯6: ä¼˜åŒ–ç®—æ³•

### é—ä¼ ç®—æ³•å®Œæ•´å®ç°Prompt

```
è¯·ç”¨é—ä¼ ç®—æ³•æ±‚è§£ä¼˜åŒ–é—®é¢˜ã€‚

ã€é—®é¢˜å®šä¹‰ã€‘
ç›®æ ‡å‡½æ•°ï¼šã€æè¿°è¦ä¼˜åŒ–/æœ€å°åŒ–/æœ€å¤§åŒ–çš„å‡½æ•°ã€‘
å†³ç­–å˜é‡ï¼šã€å˜é‡1, å˜é‡2, ...ã€‘
å˜é‡èŒƒå›´ï¼š
  - å˜é‡1: ã€min1, max1ã€‘
  - å˜é‡2: ã€min2, max2ã€‘
  - ...

çº¦æŸæ¡ä»¶ï¼š
  - çº¦æŸ1: ã€æè¿°ã€‘
  - çº¦æŸ2: ã€æè¿°ã€‘

ã€ç®—æ³•å‚æ•°ã€‘
ç§ç¾¤å¤§å°ï¼š50
è¿­ä»£æ¬¡æ•°ï¼š100
äº¤å‰æ¦‚ç‡ï¼š0.8
å˜å¼‚æ¦‚ç‡ï¼š0.1
ç²¾è‹±ä¿ç•™ï¼šæ˜¯
éšæœºç§å­ï¼š42

ã€å®ç°æ­¥éª¤ã€‘

æ­¥éª¤1ï¼šå®šä¹‰é€‚åº”åº¦å‡½æ•°
```python
def fitness_function(individual):
    """
    é€‚åº”åº¦å‡½æ•°
    
    Parameters:
    individual: ã€æ•°ç»„æ ¼å¼çš„å†³ç­–å˜é‡ã€‘
    
    Returns:
    fitness: ã€é€‚åº”åº¦å€¼ã€‘
    penalty: ã€æƒ©ç½šé¡¹ï¼ˆå¦‚æœè¿åçº¦æŸï¼‰ã€‘
    """
    # è®¡ç®—ç›®æ ‡å‡½æ•°å€¼
    objective_value = ã€è®¡ç®—ç›®æ ‡å‡½æ•°ã€‘
    
    # æ£€æŸ¥çº¦æŸ
    penalty = 0
    if ã€è¿åçº¦æŸã€‘:
        penalty = large_penalty
    
    return objective_value - penalty
```

æ­¥éª¤2ï¼šåˆå§‹åŒ–ç§ç¾¤
```python
import numpy as np

population_size = 50
n_variables = ã€å˜é‡æ•°é‡ã€‘
lower_bounds = np.array([min1, min2, ...])
upper_bounds = np.array([max1, max2, ...])

# éšæœºåˆå§‹åŒ–
population = np.random.uniform(low=lower_bounds, 
                                high=upper_bounds,
                                size=(population_size, n_variables))
```

æ­¥éª¤3ï¼šé—ä¼ ç®—æ³•ä¸»å¾ªç¯
```python
for generation in range(100):
    # è®¡ç®—é€‚åº”åº¦
    fitness = np.array([fitness_function(ind) for ind in population])
    
    # é€‰æ‹©
    parents = selection(population, fitness, n_parents=25)
    
    # äº¤å‰
    offspring = crossover(parents, offspring_size=25)
    
    # å˜å¼‚
    offspring = mutation(offspring, mutation_rate=0.1)
    
    # ç»„åˆï¼šç²¾è‹±ä¿ç•™ + åä»£
    population = survival_selection(population, offspring, fitness)
```

æ­¥éª¤4ï¼šå¯è§†åŒ–ä¼˜åŒ–è¿‡ç¨‹
```python
plt.figure(figsize=(10, 6))
plt.plot(range(len(best_fitness)), best_fitness)
plt.xlabel('Generation')
plt.ylabel('Fitness')
plt.title('Genetic Algorithm Optimization')
plt.show()
```

ã€è¾“å‡ºè¦æ±‚ã€‘
1. å®Œæ•´ä»£ç 
2. æœ€ä¼˜è§£å’Œæœ€ä¼˜å€¼
3. æ”¶æ•›æ›²çº¿
4. å‚æ•°æ•æ„Ÿæ€§åˆ†æ
```

### ç²’å­ç¾¤ä¼˜åŒ–å®Œæ•´å®ç°Prompt

```
è¯·ç”¨ç²’å­ç¾¤ç®—æ³•(PSO)æ±‚è§£ä¼˜åŒ–é—®é¢˜ã€‚

ã€é—®é¢˜å®šä¹‰ã€‘
ç›®æ ‡å‡½æ•°ï¼šã€æè¿°ã€‘
å†³ç­–å˜é‡ï¼šã€å˜é‡åˆ—è¡¨ã€‘
å˜é‡èŒƒå›´ï¼šã€æ¯ä¸ªå˜é‡çš„minå’Œmaxã€‘

ã€ç®—æ³•å‚æ•°ã€‘
ç²’å­æ•°é‡ï¼š30
æœ€å¤§è¿­ä»£ï¼š100
w: æƒ¯æ€§æƒé‡ã€0.9â†’0.4çº¿æ€§é€’å‡ã€‘
c1: è®¤çŸ¥å­¦ä¹ å› å­ã€2.0ã€‘
c2: ç¤¾ä¼šå­¦ä¹ å› å­ã€2.0ã€‘
éšæœºç§å­ï¼š42

ã€å®ç°æ­¥éª¤ã€‘

æ­¥éª¤1ï¼šåˆå§‹åŒ–ç²’å­ç¾¤
```python
n_particles = 30
n_variables = ã€å˜é‡æ•°ã€‘

# éšæœºåˆå§‹åŒ–ä½ç½®å’Œé€Ÿåº¦
particles = np.random.uniform(low=lower_bounds, 
                               high=upper_bounds,
                               size=(n_particles, n_variables))
velocities = np.random.uniform(-1, 1, (n_particles, n_variables))

# åˆå§‹åŒ–ä¸ªä½“æœ€ä¼˜å’Œå…¨å±€æœ€ä¼˜
personal_best_positions = particles.copy()
personal_best_fitness = np.array([fitness_function(p) for p in particles])
global_best_idx = np.argmin(personal_best_fitness)
global_best_position = particles[global_best_idx].copy()
```

æ­¥éª¤2ï¼šPSOä¸»å¾ªç¯
```python
for iteration in range(100):
    for i in range(n_particles):
        # æ›´æ–°é€Ÿåº¦
        r1, r2 = np.random.rand(2)
        velocities[i] = (w * velocities[i] +
                        c1 * r1 * (personal_best_positions[i] - particles[i]) +
                        c2 * r2 * (global_best_position - particles[i]))
        
        # æ›´æ–°ä½ç½®
        particles[i] += velocities[i]
        
        # è¾¹ç•Œå¤„ç†
        particles[i] = np.clip(particles[i], lower_bounds, upper_bounds)
        
        # è®¡ç®—é€‚åº”åº¦
        fitness = fitness_function(particles[i])
        
        # æ›´æ–°ä¸ªä½“æœ€ä¼˜
        if fitness > personal_best_fitness[i]:
            personal_best_fitness[i] = fitness
            personal_best_positions[i] = particles[i].copy()
        
        # æ›´æ–°å…¨å±€æœ€ä¼˜
        if fitness > personal_best_fitness[global_best_idx]:
            global_best_idx = i
            global_best_position = particles[i].copy()
```

æ­¥éª¤3ï¼šå¯è§†åŒ–
```python
plt.figure(figsize=(10, 6))
plt.plot(best_fitness_history)
plt.xlabel('Iteration')
plt.ylabel('Best Fitness')
plt.title('PSO Optimization')
plt.show()
```

ã€è¾“å‡ºè¦æ±‚ã€‘
1. å®Œæ•´ä»£ç 
2. æœ€ä¼˜è§£
3. æ”¶æ•›æ›²çº¿
4. ç²’å­è¿åŠ¨åŠ¨ç”»ï¼ˆå¯é€‰ï¼‰
```

---

## ğŸ“Š åœºæ™¯7: é›†æˆå­¦ä¹ 

### Votingé›†æˆå®Œæ•´å®ç°Prompt

```
è¯·å®ç°Votingé›†æˆå­¦ä¹ ã€‚

ã€åŸºæ¨¡å‹ã€‘
æ¨¡å‹1ï¼šLogisticRegression
æ¨¡å‹2ï¼šRandomForest
æ¨¡å‹3ï¼šGradientBoosting
æ¨¡å‹4ï¼šSVC
æ¨¡å‹5ï¼šKNeighborsClassifier

ã€é›†æˆæ–¹å¼ã€‘
voting: 'hard'ï¼ˆå¤šæ•°æŠ•ç¥¨ï¼‰/ 'soft'ï¼ˆåŠ æƒå¹³å‡ï¼‰

ã€å®ç°æ­¥éª¤ã€‘
```python
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

# å®šä¹‰åŸºæ¨¡å‹
model1 = LogisticRegression(random_state=42)
model2 = RandomForestClassifier(n_estimators=100, random_state=42)
model3 = SVC(probability=True, random_state=42)

# åˆ›å»ºVotingåˆ†ç±»å™¨
voting_clf = VotingClassifier(
    estimators=[
        ('lr', model1),
        ('rf', model2),
        ('svc', model3)
    ],
    voting='soft'  # ä½¿ç”¨æ¦‚ç‡åŠ æƒ
)

# è®­ç»ƒ
voting_clf.fit(X_train, y_train)

# é¢„æµ‹å’Œè¯„ä¼°
y_pred = voting_clf.predict(X_test)
```

ã€å¯¹æ¯”ã€‘
å¯¹æ¯”å•æ¨¡å‹å’Œé›†æˆæ¨¡å‹çš„æ€§èƒ½
```

### Stackingé›†æˆå®Œæ•´å®ç°Prompt

```
è¯·å®ç°Stackingé›†æˆã€‚

ã€åŸºæ¨¡å‹ã€‘
åŸºæ¨¡å‹1-3ï¼šã€åˆ—å‡ºåŸºæ¨¡å‹ã€‘
å…ƒå­¦ä¹ å™¨ï¼šLogisticRegression

ã€å®ç°æ­¥éª¤ã€‘
```python
from sklearn.ensemble import StackingClassifier

# å®šä¹‰åŸºæ¨¡å‹
base_estimators = [
    ('lr', LogisticRegression()),
    ('rf', RandomForestClassifier(n_estimators=100)),
    ('svm', SVC(probability=True))
]

# å®šä¹‰å…ƒå­¦ä¹ å™¨
meta_estimator = LogisticRegression()

# åˆ›å»ºStackingåˆ†ç±»å™¨
stacking_clf = StackingClassifier(
    estimators=base_estimators,
    final_estimator=meta_estimator,
    cv=5  # 5æŠ˜äº¤å‰éªŒè¯
)

# è®­ç»ƒ
stacking_clf.fit(X_train, y_train)
```

ã€è¯„ä¼°ã€‘
å¯¹æ¯”åŸºæ¨¡å‹å’ŒStackingæ¨¡å‹
```

---

## ğŸ’¡ ç®—æ³•é€‰æ‹©é€ŸæŸ¥è¡¨

| é—®é¢˜ç±»å‹ | æ¨èç®—æ³• | ä¼˜å…ˆçº§ |
|---------|---------|--------|
| äºŒåˆ†ç±» | é€»è¾‘å›å½’ã€éšæœºæ£®æ—ã€XGBoost | 1, 2, 3 |
| å¤šåˆ†ç±» | éšæœºæ£®æ—ã€XGBoostã€LightGBM | 1, 2, 3 |
| å›å½’ | çº¿æ€§å›å½’ã€Ridgeã€Lassoã€XGBoost | 1, 2, 3 |
| èšç±» | K-Meansã€å±‚æ¬¡èšç±»ã€DBSCAN | 1, 2, 3 |
| é™ç»´ | PCAã€t-SNEã€UMAP | 1, 2, 3 |
| æ—¶åºé¢„æµ‹ | ARIMAã€Prophetã€LSTM | 1, 2, 3 |
| ä¼˜åŒ– | é—ä¼ ç®—æ³•ã€PSOã€æ¨¡æ‹Ÿé€€ç« | 1, 2, 3 |

---

**æ–‡ä»¶ç»“æŸ**

æœ¬æ–‡ä»¶æ¶µç›–äº†å»ºæ¨¡ç®—æ³•çš„å®Œæ•´å®ç°ï¼ŒåŒ…æ‹¬ç®—æ³•é€‰æ‹©å»ºè®®ã€æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€æ—¶é—´åºåˆ—ã€èšç±»ã€ä¼˜åŒ–å’Œé›†æˆå­¦ä¹ ï¼Œæ¯ä¸ªæ¨¡æ¿éƒ½åŒ…å«è¯¦ç»†çš„å®ç°æ­¥éª¤å’Œå‚æ•°è¯´æ˜ã€‚
